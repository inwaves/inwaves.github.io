Inference-time Intervention
Eliciting Truthful Answers from a Language Model

Li et al. 2023

Setting the stage
‚Ä¢ Language models become ubiquitous in the span of 5 years
‚Ä¢ They‚Äôre unreasonably e ective at general purpose tasks, so long as you scale
them and their datasets to be large enough

‚Ä¢ We also have a set of post-training techniques that make them even more

powerful and accessible to the end-user (RLHF, SFT on downstream tasks,
prompting techniques)

‚Ä¢ But LMs occasionally output false statements, ranging from small mistakes to

ff

full outright ‚Äúhallucinations‚Äù ‚Äì elaborate stories that are factually incorrect

Truthfulness
‚Ä¢ Truth is a di cult concept to pin down, especially as a training objective
‚Ä¢ Most of the techniques we use have subtle failure modes
‚Ä¢ imitation learning? you might learn common misconceptions
‚Ä¢ RLHF? humans may not be able to distinguish the truth

ffi

‚Ä¢ How do you get models to output true things?

Truthfulness
‚Ä¢ It turns out that models encode something like ‚Äútruth‚Äù in their internal
representations

‚Ä¢ it makes sense: as a feature, truth is useful in many types of tasks
‚Ä¢ We know this, because models are often able to critique their own answers
after the fact (generator-discriminator gap)

‚Ä¢ If they didn‚Äôt contain a concept ~‚Äútruth‚Äù this would not be possible
‚Ä¢ It just isn‚Äôt straightforward to get models themselves to use this latent
structure to generate true answers

OK, now what?
‚Ä¢ Bypass model outputs completely, and use the internal representation to
generate an output

‚Ä¢ This is what Burns et al. 2022 do with contrast-consistent search
‚Ä¢ The idea is that wherever ‚Äútruth‚Äù is represented internally, it has to follow
logical consistency in a way that other features do not

‚Ä¢ we can nd that in a non-supervised way, with pairs of contrasting
statements

fi

‚Ä¢ ‚Ä¶

OK, now what?
‚Ä¢ What if you could instead
‚Ä¢ detect the ‚Äútruth‚Äù direction within internal activations
‚Ä¢ make models more truthful overall by shifting activations along that
direction?

‚Ä¢ This is what inference-time intervention is, in a nutshell

Detecting truth
‚Ä¢ Given a transformer-based language model, a logical place to look for truth as
a feature is in the residual stream

‚Ä¢ conceptually, each layer reads from the residual stream, does some
operation, and writes it back to the stream

‚Ä¢ Usually one transformer block is one multi-headed attention layer followed by
an MLP/fully-connected layer

‚Ä¢ let‚Äôs consider the outputs of individual attention heads in the multi-headed
attention layer

Detecting truth
‚Ä¢ The output at layer l + 1 is:
H

h
h
h
xl+1 = xl +
Ql Attl (Pl xl)
‚àë
‚Ä¢
h=1

‚Ä¢ P projects the input to a D-dimensional head space, Q projects it back to the
hidden dimension

‚Ä¢ Att is a shorthand for the attention mechanism ‚Äì the speci cs are not
important here

fi

‚Ä¢ there are h = 1,H attention heads

Detecting truth
‚Ä¢ For each of these attention heads, we can train a linear probe on their outputs
‚Ä¢ A linear probe is a simple classi er
h
h
D
‚Ä¢ pŒ∏(xl ) = œÉ(‚ü®Œ∏, xl ‚ü©), with œÉ denoting the sigmoid function, and Œ∏ ‚àà ‚Ñù a

trainable weight

‚Ä¢ We train this probe on a modi ed TruthfulQA dataset, on pairs

fi

fi

‚Ä¢ (question + answer, truth value)

fi

This is the Way
‚Ä¢ after each probe is trained, test it on the validation set
‚Ä¢ some heads get high accuracy, some don‚Äôt ‚Äì those which have high accuracy
are involved in generating truthful answers

h
‚Ä¢ for trained probes, we can think of the direction of the parameter Œ∏l as the

rst truthful direction

‚Ä¢ i.e. the direction along which true and false are most separable
‚Ä≤

‚Ä¢ you can train a second linear probe pŒ∏‚Ä≤ with the constraint that Œ∏ ‚ä• Œ∏ to get a
Ôøº

Ôøº

second direction (very similar to PCA)

Finally, inference-time intervention
‚Ä¢ Given these directions de ned by Œ∏, Œ∏‚Ä≤, we can for each attention head shift the

activations to make the model more truthful, by modifying the formula from earlier:
H

h
h
h h
h
xl+1 = xl +
Ql (Attl (Pl xl) + Œ±œÉl Œ∏l )
‚àë
‚Ä¢
h=1

‚Ä¢ Here, refers to the standard deviation of the activations in xl ‚Äì we would not

want to shift it by too much, so we refer to the initial distribution for a sensible
value

fi

ùúé

Ôøº

‚Ä¢ Œ± is a hyperparameter that controls the strength of the intervention

Finally, inference-time intervention
‚Ä¢ In practice, we don‚Äôt update all attention heads; we take the top-K heads
which are most ‚Äúactive‚Äù in truthfulness, as measured by their probes‚Äô
validation accuracies

‚Ä¢ high validation accuracy means that probe is classifying truth-falsehood;
low means it‚Äôs doing something else

‚Ä¢ The reason to do this is that sparse interventions are less likely to harm overall
model performance

‚Ä¢ We don‚Äôt want truthful-but-uninformative

Results

Conclusion
‚Ä¢ We now have a drop-in change to make models more truthful
‚Ä¢ you can apply this to any LM where you have access to the weights and
activations

‚Ä¢ We have another piece of evidence that models do encode latent structure
that corresponds to real-world concepts, like truth

‚Ä¢ it looks like it‚Äôs not just a direction, but a subspace of our residual stream/
activation space

Limitations
‚Ä¢ Supervised method: you need a few annotated data points to train the linear
probes

‚Ä¢ not so many, since e ectiveness plateaus early
‚Ä¢ Has to be sparse, otherwise overall performance is worse (Table 5)
‚Ä¢ Fundamental trade-o between truthfulness and informativeness (Figure 6)
‚Ä¢ Generalisation to other datasets is key to this being a useful intervention

ff

ff

‚Ä¢ seems like performance not harmed on MMLU, TriviaQA, but more needed

Limitations
‚Ä¢ The paper reports cross-entropy and KL-divergence as metrics for how much ITI
changes model behaviour

‚Ä¢ lower is better ‚Äì the model is more truthful, but not less capable in other ways
‚Ä¢ but no contextualisation of KL/CE values reported: how much is a lot?
‚Ä¢ also, these are not su cient, we should check that impact on downstream tasks is
not harmed by ITI

‚Ä¢ seems like most of the improvement in the best case (few-shot + ITI) comes from
few-shot prompting

ffi

‚Ä¢ 30.5% to 49.5% with just few-shot, 43.5% with just ITI, 51.4% with both

