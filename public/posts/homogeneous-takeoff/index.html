<!DOCTYPE html>

<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="color-scheme" content="light dark">
  
  
    
  
  <meta name="description" content="Connecting recent ML research on variance and simplicity bias to AI takeoff scenarios and model homogeneity">

  <title>Why you might expect homogeneous takeoff: evidence from ML research</title>
  <link rel="icon" type="image/png" sizes="32x32" href="https://inwaves.github.io/img/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="https://inwaves.github.io/img/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="https://inwaves.github.io/img/apple-touch-icon.png">
  
  <style>

  /* light mode colors */
  body {
    --primary-color: #5871a2;
    --primary-pale-color: #5871a233;
    --primary-decoration-color: #5871a210;
    --bg-color: #ffffff;
    --text-color: #2f3030;
    --text-pale-color: #767676;
    --text-decoration-color: #a9a9a9;
    --highlight-mark-color: #5f75b020;

    --callout-note-color: #5871a2;
    --callout-tip-color: #268556;
    --callout-important-color: #885fc9;
    --callout-warning-color: #ab6632;
    --callout-caution-color: #c64e4e;
  }

  /* dark mode colors */
  body.dark {
    --primary-color: #6f8fd1;
    --primary-pale-color: #6f8fd166;
    --primary-decoration-color: #6f8fd112;
    --bg-color: #1c1c1c;
    --text-color: #c1c1c1;
    --text-pale-color: #848484;
    --text-decoration-color: #5f5f5f;
    --highlight-mark-color: #8296cb3b;

    --callout-note-color: #6f8fd1;
    --callout-tip-color: #47976f;
    --callout-important-color: #9776cd;
    --callout-warning-color: #ad7a52;
    --callout-caution-color: #d06161;
  }

  /* typography */
  body {
    --main-font: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
    --code-font: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace;
    --homepage-max-width: 768px;
    --main-max-width: 768px;
    --avatar-size: 60px;
    --font-size: 16px;
    --line-height: 1.75;
    --img-border-radius: 0px;
    --detail-border-radius: 0px;
    --dark-mode-img-brightness: 0.75;
    --dark-mode-chart-brightness: 0.75;
    --inline-code-border-radius: 2px;
    --inline-code-bg-color: var(--primary-decoration-color);
    --block-code-border-radius: 0px;
    --block-code-border-color: var(--primary-color);
    --detail-border-color: var(--primary-color);
  }

</style>

  <link rel="stylesheet" href="https://inwaves.github.io/main.css">
  

<link id="hl" rel="stylesheet" type="text/css" href="/hl-light.css" />


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" integrity="sha384-HORx6nWi8j5/mYA+y57/9/CZc5z8HnEw4WUZWy5yOn9ToKBv1l58vJaufFAn9Zzi" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                { left: '$$', right: '$$', display: true },
                { left: '$', right: '$', display: false },
                { left: '\\(', right: '\\)', display: false },
                { left: '\\[', right: '\\]', display: true }
            ],
            throwOnError: false
        });
    });
</script>


  
</head>

<body class="post">
  
  <script>
    const theme = sessionStorage.getItem('theme');
    const match = window.matchMedia("(prefers-color-scheme: dark)").matches
    if ((theme && theme == 'dark') || (!theme && match)) {
      document.body.classList.add('dark');
      const hl = document.querySelector('link#hl');
      if (hl) hl.href = 'https://inwaves.github.io/hl-dark.css';
    }
  </script>
  
  
<div id="wrapper">
  <div id="blank"></div>
  <aside>
    
    
    <nav>
      <ul>
        
        <li>
          <a class="h2" href="#introduction">Introduction</a>
          
          <ul>
            
            <li>
              <a class="h3" href="#tl-dr">TL; DR</a>
            </li>
            
          </ul>
          
        </li>
        
        <li>
          <a class="h2" href="#what-s-homogeneous-take-off">What&#x27;s homogeneous take-off?</a>
          
        </li>
        
        <li>
          <a class="h2" href="#argument-from-variance">Argument from variance</a>
          
          <ul>
            
            <li>
              <a class="h3" href="#bias-variance-decomposition">Bias-variance decomposition</a>
            </li>
            
            <li>
              <a class="h3" href="#double-descent-and-variance">Double descent and variance</a>
            </li>
            
            <li>
              <a class="h3" href="#how-might-this-turn-out-to-be-false">How might this turn out to be false?</a>
            </li>
            
            <li>
              <a class="h3" href="#things-i-don-t-yet-understand">Things I don&#x27;t yet understand</a>
            </li>
            
            <li>
              <a class="h3" href="#open-questions">Open questions</a>
            </li>
            
          </ul>
          
        </li>
        
        <li>
          <a class="h2" href="#argument-from-simplicity-bias">Argument from simplicity bias</a>
          
          <ul>
            
            <li>
              <a class="h3" href="#how-might-this-turn-out-to-be-false-1">How might this turn out to be false?</a>
            </li>
            
          </ul>
          
        </li>
        
        <li>
          <a class="h2" href="#summary">Summary</a>
          
        </li>
        
      </ul>
    </nav>
    
    
    <button id="back-to-top" aria-label="back to top">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-up"><line x1="12" y1="19" x2="12" y2="5"></line><polyline points="5 12 12 5 19 12"></polyline></svg>

    </button>
    
  </aside>
  <main>
    
<header>
  <nav>
    <a id="back-link" href="https:&#x2F;&#x2F;inwaves.github.io&#x2F;posts">← Back</a>
  </nav>
</header>


    <div>
      
      
      
      
      <div id="copy-cfg" style="display: none;" data-copy-icon="&lt;svg xmlns=&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&quot; viewBox=&quot;0 0 24 24&quot; width=&quot;18&quot; height=&quot;18&quot;&gt;&lt;path d=&quot;M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z&quot; fill=&quot;currentColor&quot;&gt;&lt;&#x2F;path&gt;&lt;&#x2F;svg&gt;
" data-check-icon="&lt;svg xmlns=&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&quot; viewBox=&quot;0 0 24 24&quot; width=&quot;18&quot; height=&quot;18&quot;&gt;&lt;path d=&quot;M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z&quot; fill=&quot;currentColor&quot;&gt;&lt;&#x2F;path&gt;&lt;&#x2F;svg&gt;
"></div>
      
      <article class="prose">
        <h1>Why you might expect homogeneous takeoff: evidence from ML research</h1>
        <div id="post-info">
          <div id="date">
            <span id="publish">Jul 17, 2022</span>
            </div>

          
        </div>

        
        

        

        <p><em>The below is a cross-post from <a rel="nofollow noreferrer" href="https://www.alignmentforum.org/posts/RQn45KzN5cojLLb3L/why-you-might-expect-homogeneous-take-off-evidence-from-ml">the alignment forum</a>.</em></p>
<h1 id="introduction">Introduction<a class="zola-anchor" href="#introduction" aria-label="Anchor link for: introduction" style="visibility: hidden;"></a>
</h1>
<p>This article aims to draw a connection between recent ML research and the claim that future advanced AI systems may be homogenous. First, I briefly review <a rel="nofollow noreferrer" href="https://www.alignmentforum.org/posts/mKBfa8v4S9pNKSyKK/homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios">this article</a>, where the idea of homogenous take-off is introduced. Then, I outline two different arguments why you might update in the direction of homogenous take-off. For each of the arguments I mention key uncertainties that I have about the argument itself, as well as broader open questions.</p>
<h2 id="tl-dr">TL; DR<a class="zola-anchor" href="#tl-dr" aria-label="Anchor link for: tl-dr" style="visibility: hidden;"></a>
</h2>
<p>I present two reasons to believe that as models become larger they also become more homogenous, i.e. they behave more similarly to each other:</p>
<ul>
<li>Variance between models behaves unimodally in the overparameterised regime: it peaks around the interpolation threshold, then decreases monotonically. Decreased variance means that models make similar predictions across different training runs (captured as variance from initialisation) and different sampling of the training data (variance from sampling);</li>
<li>Neural networks have a strong simplicity bias even before training, which might mean that multiple training runs with different hyperparameters, initialisation schemes etc. result in essentially the same model.</li>
</ul>
<p>I've somewhat updated in the direction of homogenous take-off as a result of these arguments, though I think that there are still ways in which it's unclear if e.g. decreasing variance with size rules out heterogeneity.</p>
<h1 id="what-s-homogeneous-take-off">What's homogeneous take-off?<a class="zola-anchor" href="#what-s-homogeneous-take-off" aria-label="Anchor link for: what-s-homogeneous-take-off" style="visibility: hidden;"></a>
</h1>
<p>There are several axes along which different AI takeoff scenarios could differ: speed, continuity, and number of main actors. <a rel="nofollow noreferrer" href="https://www.alignmentforum.org/posts/mKBfa8v4S9pNKSyKK/homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios">Homogeneity vs. heterogeneity in AI takeoff scenarios</a> introduces a new way to look at a potential take-off, through the lens of model <strong>homogeneity</strong>. Homogeneity intuitively refers to how similar models are at any given time given some definition of similarity. We might specifically refer to homogeneity with regards to alignment, which, again intuitively, means "models are more or less aligned to the same degree" (or: "aligned models will not coexist with unaligned models").</p>
<p>More formally we mean something like models having similar properties, e.g. alignment properties. In my mind, an alignment property might be something like "corrigibility" or "truthfulness", though it's unclear to me to what extent two models which are, say, truthful, are also homogenous. I think working toward a clearer, more precise definition of homogeneity is probably useful in determining what actually counts as evidence for homogenous systems being more likely, though I don't try to do so in this write-up.</p>
<p>The article sets out a list of arguments supporting the idea of homogenous take-off, which I parse as "evidence from the economics of large scale machine learning". Without going into too much detail – I recommend reading the original article for the full arguments –, these are:</p>
<ol>
<li><strong>Training a model is more expensive than running it</strong>. This is a relatively straightforward claim which extrapolates from the landscape we have today, where some large language models reportedly have had training budgets in the millions of US dollars, with comparatively little cost to run inference/serve the models themselves once trained.</li>
<li>**Training models from scratch is not competitive once the first advanced system is released. **To me this follows from 1., in the sense that if it is economically useful to deploy more than one model simultaneously, it's likely that the additional models will be copies of the original (perhaps fine-tuned on different tasks) rather than new models trained from scratch.</li>
<li>**Copying is more competitive than (untested) alternative approaches. **Here I think it's worth disentangling two ways of copying:
<ol>
<li><strong>Direct copying</strong> of the first advanced system, possibly by using the same weights, or by running the exact same training process. There are reasons to believe that direct copying might not be possible or even desirable, since e.g. states might not want to use a competing state's advanced system.</li>
<li><strong>Indirect copying</strong> is using the same techniques as the creators of the original system, but not identical training runs/hyperparameters. This scenario seems more likely to me, and it's here where the arguments I present on variance/simplicity bias are most important, since they show that different runs may not necessarily result in different models.</li>
</ol>
</li>
<li>**Homogeneity is preserved during initial takeoff. **Here the argument is that later generations of AIs will also be homogenous, at least with respect to alignment. This is because we either use the first generation systems to align the next generations, or we use ~the same techniques we used on the 1st generation to align the next generations. The idea is that both approaches result in systems with the same alignment properties. It's unclear to me whether the same kind of argument holds for something other than alignment properties – and if not, why not.</li>
</ol>
<p>In this article I want to present two technical arguments from recent ML research that support the idea of homogenous take-off. First, as models become larger, variance between models decreases. Second, neural networks seem to be biased toward simple solutions even before training.</p>
<h1 id="argument-from-variance">Argument from variance<a class="zola-anchor" href="#argument-from-variance" aria-label="Anchor link for: argument-from-variance" style="visibility: hidden;"></a>
</h1>
<h2 id="bias-variance-decomposition">Bias-variance decomposition<a class="zola-anchor" href="#bias-variance-decomposition" aria-label="Anchor link for: bias-variance-decomposition" style="visibility: hidden;"></a>
</h2>
<p>One of the main practical insights of statistical learning theory is related to the bias-variance decomposition of mean squared error. In this section I'll be introducing the concepts of bias and variance and discussing their importance in the classical and the overparameterised regimes. I'll be using the notation from <a rel="nofollow noreferrer" href="https://proceedings.neurips.cc/paper/2020/file/7d420e2b2939762031eed0447a9be19f-Paper.pdf">Adlam &amp; Pennington, 2020</a>.</p>
<p>The main idea is that for a supervised learning task where a model ($$\hat{y}$$) is minimising mean squared error on a training set $$\mathcal{D}<em>{tr}$$, we can decompose the on a test point $$x \in \mathcal{D}</em>{te}$$  as:</p>
<p>$$[\mathbb{E}[\hat y(x) - y(x)]^2 = (\mathbb{E} \hat y(x) - \mathbb{E} y(x))^2 + \mathbb{V}[\hat y(x)] + \mathbb{V}[y(x)]]$$</p>
<p>where \(y(x)\) is the ground truth. The first term is the squared bias, the second is the variance and the third is irreducible noise in the test data. The randomness in these variables is canonically taken to come from sampling noise, though as we'll see shortly there are other sources too.</p>
<p>Looking at the terms more closely, bias is how much the average prediction – across models trained on different realisations of the training set – differs from the ground truth. Bias is typically interpreted as error resulting from incorrect assumptions about the data, a phenomenon otherwise known as underfitting. For example, trying to interpolate a polynomial function with a linear model leads to high bias.</p>
<p>The second term, variance, refers to how much each individual model's prediction differs from the average prediction, and has historically been tied to overfitting to noise. If models' predictions on the test set vary widely, it's because they've all learned spurious correlations in their specific training set which do not hold on test data. The last term is irreducible noise in the test set.</p>
<p>Ideally, we'd like both bias and variance to be small. However, in practice, bias and variance seem to trade off against each other as model capacity is increased, resulting in the typical U-shaped curve for test risk in Figure 1a. This trade-off implies that to achieve the optimal test risk, a model should aim for the "sweet spot" where the trade-off is optimal. If models are larger than the optimal size, variance increases, and the result is overfitting to the training data and poor generalisation performance on test data. If they are smaller, they underfit the training data and do relatively poorly on all datasets.</p>
<p>This is the received wisdom from the bias-variance decomposition in the "classical" regime, and the dominating view pre-deep learning. This is mostly invalidated by deep neural networks, which generalise well despite being very large relative to the datasets they are trained on. The phenomenon of double descent (see e.g. <a rel="nofollow noreferrer" href="https://www.pnas.org/doi/full/10.1073/pnas.1903070116">Belkin et al., 2019</a>) illustrates the capacity of large models to interpolate (i.e. perfectly fit) the data, and yet perform well to unseen data. In Figure 1b, as model size is increased, we move from the classical U-shaped risk curve to a peak at the interpolation threshold, with risk decreasing monotonically past the peak to values that are below the previous minimum.</p>
<p><img src="/images/adlam_pennington2020.jpeg" alt="Figure 1" /></p>
<p>**Figure 1. From **<a rel="nofollow noreferrer" href="https://www.pnas.org/doi/full/10.1073/pnas.1903070116"><strong>Belkin et al., 2019</strong></a>. A visual representation of (a) the underparameterised regime, where the bias-variance trade-off occurs as models increase in capacity (here formalised as the size of a hypothesis class \(\mathcal{H}\)) and (b) the modern overparameterised regime, where test risk decreases despite models which are large relative to the size of their training datasets.</p>
<h2 id="double-descent-and-variance">Double descent and variance<a class="zola-anchor" href="#double-descent-and-variance" aria-label="Anchor link for: double-descent-and-variance" style="visibility: hidden;"></a>
</h2>
<p>Several recent papers (<a rel="nofollow noreferrer" href="https://proceedings.mlr.press/v119/yang20j/yang20j.pdf">Yang et al., 2020</a>, <a rel="nofollow noreferrer" href="https://www.jmlr.org/papers/volume22/20-1211/20-1211.pdf">Lin &amp; Dobriban, 2021</a>, <a rel="nofollow noreferrer" href="https://proceedings.neurips.cc/paper/2020/file/7d420e2b2939762031eed0447a9be19f-Paper.pdf">Adlam &amp; Pennington, 2020</a>) examine double descent through the lens of the bias-variance decomposition. Broadly speaking, the main finding is that variance behaves unimodally – it increases, peaks, and then decreases monotonically. Depending on the magnitude of bias relative to variance, several test risk curves can be obtained, including double descent – see Figure 2. below.</p>
<p>The important observation here is that as models increase in size, variance decreases. To zoom out a bit, remember that variance captures the degree to which models differ in their predictions across different training runs. Decreasing variance means that models become more homogenous. In a sense, this follows directly from double descent, since we know that bias decreases with size and that after the interpolation threshold test risk decreases monotonically.</p>
<p><img src="/images/yang2020.jpeg" alt="Figure 2" /></p>
<p><strong>Figure 2. From <strong><a rel="nofollow noreferrer" href="https://proceedings.mlr.press/v119/yang20j/yang20j.pdf"><strong>Yang et al., 2020</strong></a></strong>.</strong> A hypothetical test risk curve plotted against model complexity, alongside its bias and variance components. The three cases are as follows: (a) if bias dominates variance over the entire x-axis, then test risk follows a monotonic decrease; (b) if bias and variance dominate in different regimes, the test risk follows a double descent curve; (c) if variance dominates bias over the entire x-axis, then test risk is simply unimodal – without the initial decrease.</p>
<p>To try and understand what is happening with double descent, the latter two papers focus on decomposing variance into additional sources of randomness (training data sampling, parameter initialisation and label noise) and find that some components behave unimodally, while others increase up to the interpolation threshold and stay constant afterward (e.g. variance due to sampling and due to label noise, see Figure 3j).</p>
<p>There seems to not be any consensus on how to additionally decompose variance (including the order in which to condition on these sources of randomness – because conditioning isn't symmetrical). Because of this, e.g. <a rel="nofollow noreferrer" href="https://link.aps.org/pdf/10.1103/PhysRevResearch.4.013201">Rocks &amp; Mehta, 2022</a> hint that some studies are led to incorrect conclusions about the relationship between variance and double descent.</p>
<p><img src="/images/belkin2019.jpeg" alt="Figure 3" /></p>
<p><strong>Figure 3. From <strong><a rel="nofollow noreferrer" href="https://proceedings.neurips.cc/paper/2020/file/7d420e2b2939762031eed0447a9be19f-Paper.pdf"><strong>Adlam &amp; Pennington, 2020</strong></a></strong>.</strong> Different decompositions of the bias (\(B\)) and the variance (\(V\)) of a neural network with hidden layer size \(n_1\) and dataset size \(m\). Most useful is the right-most decomposition into variance from sampling (\(V_X\)), variance from initialisation (\(V_P\)) and variance from label noise (\(V_\epsilon\)), along with their interaction effects, e.g. \(V_{PX}, V_{PX\epsilon}\). In figure (j) \(B, V_X\) and \(V_{X\epsilon}\) converge to a constant value after the interpolation threshold, and their values are no longer sensitive to increase in model size. All other sources of test risk are unimodal: they peak at the interpolation threshold and decrease with model size.</p>
<p>A few side-notes:</p>
<ul>
<li>Label noise exacerbates but doesn't cause double descent, since other components of variance peak even in the absence of label noise (Fig. 3j).</li>
<li>It turns out that in particular regimes the higher-level interaction effects between different sources of variance dominate the main effects, i.e. \(V_{si} &gt; V_s &gt; V_i\), where \(V_{s}\) is variance from sampling, \(V_i\) is variance from initialisation and \(V_{si}\) is the variance from the interaction between the two.</li>
</ul>
<p>It's also worth mentioning that these studies take place in the asymptotic setting, i.e. they investigate what happens if the number of samples in the training dataset and dimensionality of the data go to infinity while maintaining a fixed ratio. <a rel="nofollow noreferrer" href="https://www.jmlr.org/papers/volume22/20-1211/20-1211.pdf">Lin &amp; Dobriban, 2021</a> find that this ratio controls the unimodal behaviour of the variance: if the ratio is below a threshold, variance peaks, then decreases; otherwise it increases.</p>
<p>If this analysis is correct, as long as we control the ratio we can ensure that models become more homogenous as they become larger. It's worth noting that this hasn't been replicated yet, to my knowledge, and that this unimodal variance explanation for double descent is not the only hypothesis, see e.g. <a rel="nofollow noreferrer" href="https://proceedings.neurips.cc/paper/2021/file/f754186469a933256d7d64095e963594-Paper.pdf">Kuzborskij et al., 2021</a> for an account of DD related to the smallest positive eigenvalue of the feature covariance matrix.</p>
<h2 id="how-might-this-turn-out-to-be-false">How might this turn out to be false?<a class="zola-anchor" href="#how-might-this-turn-out-to-be-false" aria-label="Anchor link for: how-might-this-turn-out-to-be-false" style="visibility: hidden;"></a>
</h2>
<p>First, it's possible that the findings from analysis of variance are not robust to changes in architecture or learning task, though at least <a rel="nofollow noreferrer" href="https://proceedings.mlr.press/v119/yang20j/yang20j.pdf">Yang et al. 2020</a> seem to cover quite a few experimental set-ups (including changing architecture and dataset as well as other potentially-less-impactful training hyperparameters). This means that it might be useful to do more experiments to probe the robustness of these findings. If they turn out to scale well/hold across architectures, then this is stronger evidence in favour of homogeneity.</p>
<p>Second, it could be that residual variance – variance that is not eliminated through training – is enough to invalidate the homogeneity hypothesis, in the sense that residual variance could lead to different behaviour/properties of models that exist at the same time. I'm not sure how likely this is, given that the residual variances seem to be quite small – on the order of 10^{-3} according to <a rel="nofollow noreferrer" href="https://proceedings.neurips.cc/paper/2020/file/7d420e2b2939762031eed0447a9be19f-Paper.pdf">Adlam &amp; Pennington, 2020</a> – though of course here the threshold is unknown. (How much variance implies heterogeneity doesn't seem to be a well-posed question.)</p>
<p>I don't have a good idea for how to resolve this uncertainty. It seems to me that unless we can find a more precise definition of homogeneity, we can't say exactly how much residual variance matters.</p>
<h2 id="things-i-don-t-yet-understand">Things I don't yet understand<a class="zola-anchor" href="#things-i-don-t-yet-understand" aria-label="Anchor link for: things-i-don-t-yet-understand" style="visibility: hidden;"></a>
</h2>
<ul>
<li>How does the fixed-design/random-design decomposition affect the result? For example see <a rel="nofollow noreferrer" href="https://arxiv.org/pdf/1903.08560">Hastie et al., 2022</a>.</li>
<li>Lots of these experiments use random features, and it's unclear to me why this is more appropriate/easy to analyse than shallow neural networks, which presumably are closer to what we care about.</li>
<li>Where does the variance from optimisation fit in? Is it the same as variance from initialisation, which is where the optimiser starts? E.g. <a rel="nofollow noreferrer" href="https://arxiv.org/pdf/1810.08591.pdf">Neal et al., 2018</a> mention variance due to optimisation, but they don't study how bias and variance change during training.
<ul>
<li>They point to variance from optimisation as encompassing: random initialisation and stochastic mini-batching, but they also say that their results hold even with batch gradient descent.</li>
</ul>
</li>
</ul>
<h2 id="open-questions">Open questions<a class="zola-anchor" href="#open-questions" aria-label="Anchor link for: open-questions" style="visibility: hidden;"></a>
</h2>
<ul>
<li>Should we expect "prediction" homogeneity to translate to alignment properties?</li>
<li>Why does variance have unimodal behaviour? It might be worth replicating the experiments in <a rel="nofollow noreferrer" href="https://www.jmlr.org/papers/volume22/20-1211/20-1211.pdf">Lin &amp; Dobriban, 2021</a> where they use the parameterisation level and data aspect ratio to control the variance.
<ul>
<li><a rel="nofollow noreferrer" href="https://proceedings.mlr.press/v119/yang20j/yang20j.pdf">Yang et al., 2020</a> conjecture that it's regularisation that leads to variance decreasing past the peak, though this seems like a broad remark that does not add much useful information.</li>
</ul>
</li>
</ul>
<h1 id="argument-from-simplicity-bias">Argument from simplicity bias<a class="zola-anchor" href="#argument-from-simplicity-bias" aria-label="Anchor link for: argument-from-simplicity-bias" style="visibility: hidden;"></a>
</h1>
<p>We have good empirical evidence that neural networks are biased toward simple functions which fit the data. There's no consensus on the mechanism behind this bias, but there are lots of competing explanations.</p>
<p>One recent such explanation is that their parameter-function maps are biased toward low-complexity functions; that is, even before training NN architectures induce a strong preference for simplicity. See <a rel="nofollow noreferrer" href="https://www.lesswrong.com/posts/YSFJosoHYFyXjoYWa/why-neural-networks-generalise-and-why-they-are-kind-of">this LW article</a> for a jumping-off point, or go directly to the technical papers: <a rel="nofollow noreferrer" href="https://arxiv.org/pdf/1805.08522">Valle-Perez, Camargo &amp; Louis, 2018</a>, <a rel="nofollow noreferrer" href="https://arxiv.org/pdf/1909.11522">Mingard et al., 2019</a>, <a rel="nofollow noreferrer" href="https://www.jmlr.org/papers/volume22/20-676/20-676.pdf">Mingard et al., 2021</a>.</p>
<p>If this analysis is correct, then various proposed mechanisms for why DNNs generalise which are related to optimiser choice or hyperparameter tuning are only small (or "second-order") deviations from the posterior \(P_B(f|S)\) whose bias is essentially induced by the prior \(P(f)\).</p>
<p>This might lead you to believe that given a fixed architecture, different initialisation schemes, optimisers, hyperparameters etc. do not contribute substantially to the properties that the trained system has, or, differently put, that different experimental setups do not result in systems which differ in the ways we care about.</p>
<p>This is consistent with the finding that variance decreases with scale, at least if we interpret the findings in Section 5 of <a rel="nofollow noreferrer" href="https://arxiv.org/pdf/1909.11522">Mingard et al., 2019</a> to mean that adding more layers results in stronger bias toward simple functions. I'd be excited about work that directly connects these two insights, especially since we don't necessarily know yet why variance is unimodal.</p>
<p>I'm a bit more sceptical that this line of argument supports homogeneity directly, mostly because I don't think that a biased parameter-function map explains all the properties of models found through e.g. SGD (nor do I think the Mingard et al. papers make these claims). If the influence of specific training hyperparameters is enough to induce heterogeneity between runs – again, only gesturing at the concept of heterogeneity rather than defining it – then even if the parameter-function map hypothesis of generalisation is true, it's evidence <em>against</em> homogeneity.</p>
<h2 id="how-might-this-turn-out-to-be-false-1">How might this turn out to be false?<a class="zola-anchor" href="#how-might-this-turn-out-to-be-false-1" aria-label="Anchor link for: how-might-this-turn-out-to-be-false-1" style="visibility: hidden;"></a>
</h2>
<ul>
<li><strong>The biased prior drives most of the inductive bias, but it doesn't explain everything.</strong> Even if details about the training setup do not account for most of DNNs' capacity to generalise, they may still account for some particular property which is relevant from an alignment perspective;</li>
<li><strong>Simplicity is not the same as homogeneity.</strong> Even if all functions that NNs tend to find are simple by some measure, it doesn't mean that they are the same function. There might be some key input where two up-to-then identical functions diverge, which could lead to negative outcomes. Again, it's possible that we won't be able to prove that two such functions are the same.</li>
<li>**Objections to the biased prior hypothesis. **It could be that the biased parameter-function map account of NN generalisation does not scale to larger networks/more complex architectures/other tasks (for some discussion, see <a rel="nofollow noreferrer" href="https://www.lesswrong.com/posts/5p4ynEJQ8nXxp2sxC/parsing-chris-mingard-on-neural-networks#Scalability">this article</a>). This might mean that some other hypothesis better explains NNs' performance in the overparameterised regime – there are many related to: the stochasticity of gradient descent, the loss landscape (through basins of attraction or through flat minima), NNs' similarity to GPs, implicit regularisation and others – which might lead us to update away from model homogeneity.</li>
</ul>
<h1 id="summary">Summary<a class="zola-anchor" href="#summary" aria-label="Anchor link for: summary" style="visibility: hidden;"></a>
</h1>
<p>This article outlines two arguments from recent ML research for why homogenous take-off is a plausible story. One stems from an empirically observed decrease in variance with model size, which is consistent with the double descent phenomenon. The other is a consequence of the finding that neural networks are a priori biased toward simple functions, which means that they are likely to find solutions with similar properties regardless of the particular training parameters.</p>
<p>I think there's still work to be done on both of these arguments, and I'd be much more willing to update in favour of homogenous takeoff if the findings were more robust or we had a better understanding of e.g. why variance is unimodal. But it seems worthwhile to make this connection and get more people thinking and talking about the likelihood of homogeneity in take-off scenarios.</p>

      </article>

      
      

      
      
      <div class="giscus"></div>
      
      
    </div>

    


<footer>
  <div class="left">
    <div class="copyright">
      © 2025 Andrei Alexandru
      
      <span>|</span>
      Built with <a href="https://www.getzola.org" rel="noreferrer" target="_blank">zola</a> and <a href="https://github.com/isunjn/serene" rel="noreferrer" target="_blank">serene</a>
      
    </div>
  </div>

  <div class="right">
    
    
      
    
    
    <a id="rss-btn" href="https://inwaves.github.io/posts/feed.xml">RSS</a>
    
    

    
    
    
    <button id="theme-toggle" aria-label="theme switch">
      <span class="moon-icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18" height="18"><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill="currentColor"></path></svg>
</span>
      <span class="sun-icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18" height="18"><path d="M12 18C8.68629 18 6 15.3137 6 12C6 8.68629 8.68629 6 12 6C15.3137 6 18 8.68629 18 12C18 15.3137 15.3137 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16ZM11 1H13V4H11V1ZM11 20H13V23H11V20ZM3.51472 4.92893L4.92893 3.51472L7.05025 5.63604L5.63604 7.05025L3.51472 4.92893ZM16.9497 18.364L18.364 16.9497L20.4853 19.0711L19.0711 20.4853L16.9497 18.364ZM19.0711 3.51472L20.4853 4.92893L18.364 7.05025L16.9497 5.63604L19.0711 3.51472ZM5.63604 16.9497L7.05025 18.364L4.92893 20.4853L3.51472 19.0711L5.63604 16.9497ZM23 11V13H20V11H23ZM4 11V13H1V11H4Z" fill="currentColor"></path></svg>
</span>
    </button>
    
  </div>
</footer>




<dialog id="rss-mask">
  <div>
    <a href="https:&#x2F;&#x2F;inwaves.github.io&#x2F;posts&#x2F;feed.xml">https:&#x2F;&#x2F;inwaves.github.io&#x2F;posts&#x2F;feed.xml</a>
    
    
    <button autofocus aria-label="copy" data-link="https:&#x2F;&#x2F;inwaves.github.io&#x2F;posts&#x2F;feed.xml" data-copy-icon="&lt;svg xmlns=&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&quot; viewBox=&quot;0 0 24 24&quot; width=&quot;18&quot; height=&quot;18&quot;&gt;&lt;path d=&quot;M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z&quot; fill=&quot;currentColor&quot;&gt;&lt;&#x2F;path&gt;&lt;&#x2F;svg&gt;
" data-check-icon="&lt;svg xmlns=&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&quot; viewBox=&quot;0 0 24 24&quot; width=&quot;18&quot; height=&quot;18&quot;&gt;&lt;path d=&quot;M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z&quot; fill=&quot;currentColor&quot;&gt;&lt;&#x2F;path&gt;&lt;&#x2F;svg&gt;
" >
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18" height="18"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>

    </button>
  </div>
</dialog>



  </main>
</div>

  
<script src="/js/lightense.min.js"></script>


  <script src="https://inwaves.github.io/js/main.js"></script>
</body>

</html>
