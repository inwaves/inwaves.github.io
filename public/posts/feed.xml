<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
	<title>Posts</title>
	<subtitle>AI safety research and technical writing</subtitle>
	<link rel="self" type="application/atom+xml" href="https://inwaves.github.io/posts/feed.xml"/>
  <link rel="alternate" type="text/html" href="https://inwaves.github.io/posts/"/>
  
	<updated>2024-04-05T00:00:00+00:00</updated>
	
	<id>https://inwaves.github.io/posts/feed.xml</id>
	<entry xml:lang="en">
		<title>Understanding mixture-of-depths</title>
		<published>2024-04-05T00:00:00+00:00</published>
		<updated>2024-04-05T00:00:00+00:00</updated>
		<link rel="alternate" type="text/html" href="https://inwaves.github.io/posts/mixture-of-depths/"/>
		<id>https://inwaves.github.io/posts/mixture-of-depths/</id>
    
		<content type="html" xml:base="https://inwaves.github.io/posts/mixture-of-depths/">&lt;p&gt;Google Deepmind recently released this paper: &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.02258&quot;&gt;Mixture-of-Depths: Dynamically allocating compute in transformer-based language models&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;In a few words: relative to a vanilla transformer, you can train either (a) models with higher performance or (b) faster-to-run models with the same performance, using a technique called mixture-of-depths. Conceptually, the change is straightforward: for each transformer block, learn a routing mechanism that determines whether a given token goes through the block, or skips it – as in a residual connection.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;setting-the-stage&quot;&gt;Setting the stage&lt;a class=&quot;zola-anchor&quot; href=&quot;#setting-the-stage&quot; aria-label=&quot;Anchor link for: setting-the-stage&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;p&gt;If you&#x27;ve been watching the machine learning space in the last 5 years or so, you&#x27;ll have noticed language models becoming near-ubiquitous. And for good reason! They&#x27;re unreasonably effective at general purpose tasks, so long as you can scale them and the datasets they&#x27;re trained on to be large enough. We now also have a set of post-training techniques that make them even more capable and useful to end-users – RLHF, SFT on downstream tasks, prompt engineering – making them useful as general purpose chatbots, coding assistants, and other applications we haven&#x27;t dreamt of yet.&lt;&#x2F;p&gt;
&lt;p&gt;But the scaling part is kind of a big deal. The largest models in production today probably cost &amp;gt;$100m to train, and have a non-negligible cost to re-train. This cost is a combination of training very large models (100s of billions of parameters) on very large datasets (10s of &lt;em&gt;trillions&lt;&#x2F;em&gt; of tokens) and likely for very many epochs. On one hand, these figures are likely going down, because hardware improvements (think NVIDIA&#x27;s latest GTC) and algorithmic progress (making the models themselves more efficient) are chipping away at compute costs. On the other hand, there&#x27;s higher demand for GPUs, we&#x27;re training more and more complex systems, and there&#x27;s an increasing number of companies wanting to explore what LLMs can do for them. If you can bring down the compute cost of training and serving a large language model, the entire ecosystem benefits.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mixture-of-depths&quot;&gt;Mixture of depths&lt;a class=&quot;zola-anchor&quot; href=&quot;#mixture-of-depths&quot; aria-label=&quot;Anchor link for: mixture-of-depths&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;p&gt;The starting intuition for the &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.02258&quot;&gt;Raposo et al. 2024&lt;&#x2F;a&gt; paper is that not all tokens in a text sequence should take the same amount of compute or time when making a prediction. Some are more important than others (we know this because self-attention works well), so we can try to translate this into compute efficiency gains.&lt;&#x2F;p&gt;
&lt;p&gt;The term of art here is &quot;conditional computation&quot;, the gist of which is that you can reduce total compute used by only expending it when needed, e.g. see &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1511.06297&quot;&gt;Bengio et al. 2016&lt;&#x2F;a&gt;. One problem with this is that we might not know in advance how much compute we need when we apply conditional computation – the computation graph is &lt;em&gt;dynamic&lt;&#x2F;em&gt;. This is bad because in large scale ML systems we want to maximise hardware utilisation, preferably knowing in advance how much memory or bandwidth we&#x27;ll need for a particular computation.&lt;&#x2F;p&gt;
&lt;p&gt;To avoid dynamic computation graphs, we can pre-specify a total compute budget, which doesn&#x27;t change during training. We can tie this budget to the number of tokens in a sequence that can participate in a transformer block&#x27;s computations – let&#x27;s say $k$ of $T$ total tokens.&lt;&#x2F;p&gt;
&lt;p&gt;Given this fixed budget, we can get transformers to learn to dynamically allocate compute to each input token, in each layer, i.e. which $k$ of the $T$ total tokens will participate in each block. Because not all tokens participate in every computation, less compute is required, and the number of FLOPs used during training is lower than for a vanilla transformer.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;mod-fig1.png&quot; alt=&quot;MoD Figure 1&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The way you do this is through a routing mechanism: moving down through the network layers, each token either has the layer applied to it (just like a vanilla transformer, no changes there) or it passes through a residual connection (i.e. no computation is applied to it). The routing applies to entire transformer blocks, which comprise one attention layer, and one MLP.&lt;&#x2F;p&gt;
&lt;p&gt;Routing is controlled by per-block routers – these generate a scalar weight for each token, the intuition for which is that router&#x27;s &quot;preference&quot; for the token to participate in the computation. Given a bunch of these preference weights, identify the $k$ highest of them and select those tokens to participate in the block.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;routing-tokens&quot;&gt;Routing tokens&lt;a class=&quot;zola-anchor&quot; href=&quot;#routing-tokens&quot; aria-label=&quot;Anchor link for: routing-tokens&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;p&gt;How do you route tokens in practice?&lt;&#x2F;p&gt;
&lt;p&gt;You could do it stochastically by sampling the routing weights from a normal distribution - this turns out not to work so well - or you could learn the weights of the router as part of regular training.&lt;&#x2F;p&gt;
&lt;p&gt;Specifically, let&#x27;s say you have a set of token embeddings for a sequence of length $S$ in layer $l$:&lt;&#x2F;p&gt;
&lt;p&gt;$$X^l={x_i^l|i=\overline{1,S}}$$&lt;&#x2F;p&gt;
&lt;p&gt;Give each token embedding a router weight generated by a linear projection:&lt;&#x2F;p&gt;
&lt;p&gt;$$r_i^l = w_\theta^T x_i^l$$&lt;&#x2F;p&gt;
&lt;p&gt;Now taking $P_\beta(R^l)$ as the $\beta$-th percentile of the set of router weights $R^l$, where $\beta = 1 - \frac{k}{S}$, each block&#x27;s output for a given token is:&lt;&#x2F;p&gt;
&lt;p&gt;$$x_i^{l+1} = \begin{cases}
r_i^lf_i(\tilde X^l) + x_i^l, \quad \text{if}\ r_i^l &amp;gt; P_\beta(R^l) \
x_i^l, \quad\quad\quad\quad\quad\ \ \ \text{if}\  r_i^l &amp;lt; P_\beta(R^l)
\end{cases}$$&lt;&#x2F;p&gt;
&lt;p&gt;In short:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;we use the percentile to pick the top-k tokens;&lt;&#x2F;li&gt;
&lt;li&gt;the $f_i$ function corresponds to the transformer block;&lt;&#x2F;li&gt;
&lt;li&gt;the $\tilde X$ vector corresponds to the tokens that this token depends on (because of attention) – there are $k$ of them;&lt;&#x2F;li&gt;
&lt;li&gt;we multiply the block output by the router weight so that the latter gets added to the computational graph and acted on during backprop, taking gradients with respect to this parameter just like we do with others;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I think this is a pretty clean solution – it&#x27;s mathematically unsophisticated, and seems to work well in practice! There is just one &lt;em&gt;small&lt;&#x2F;em&gt; snag: when we use the top-$k$ weights to determine which tokens take part in a block, the decision for a token $x_i$ depends on tokens that follow it, like $x_{i+1}, x_{i+2}$ and so on! After all, the routing mechanism might just prefer them more.&lt;&#x2F;p&gt;
&lt;p&gt;This breaks autoregressive sampling, i.e. where we generate one token, then we use it to generate another, and another, left-to-right. One solution to this is to add a second classifier to each router to try and predict whether given a token as input that token will be in the top-$k$ or not.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;results&quot;&gt;Results&lt;a class=&quot;zola-anchor&quot; href=&quot;#results&quot; aria-label=&quot;Anchor link for: results&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;p&gt;It&#x27;s a good idea to go read the original paper for a nuanced interpretation of the results, but a few things seem worth mentioning here:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Optimal mixture-of-depths (MoD) transformers get lower loss than optimal baselines (i.e. optimal-compute trained vanilla transformers), and they have more parameters. This means you can either have models which for the same compute cost have lower loss&#x2F;higher performance, or you can have smaller, cheaper models that have the same loss as a more expensive, larger baseline!&lt;&#x2F;p&gt;
&lt;p&gt;One way to look at this is that for a given wall-clock time spent during training, an MoD model will get more training steps in than a vanilla model, because each step takes less time. So the MoD model trains for longer!&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The best MoD variant they found routed every other block (and left other block as in vanilla transformers), and used (by my lights) &lt;em&gt;super&lt;&#x2F;em&gt; aggressive top-k choices: only 12.5% of all tokens got processed by the routed blocks! That means 87.5% get skipped by every other transformer block.
&lt;img src=&quot;&#x2F;images&#x2F;mod-fig5.png&quot; alt=&quot;MoD Figure 5&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;You can combine mixture-of-depths and mixture-of-experts models to get an even handier acronym: MoDE. There&#x27;s two architecture ideas, one of which seems to work better than the other.
&lt;img src=&quot;&#x2F;images&#x2F;mod-fig7.png&quot; alt=&quot;MoD Figure 7&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;wrapping-up&quot;&gt;Wrapping up&lt;a class=&quot;zola-anchor&quot; href=&quot;#wrapping-up&quot; aria-label=&quot;Anchor link for: wrapping-up&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;p&gt;I like the simplicity of the routing – it&#x27;s basically a learned form of dropout, calculated per token, per block. It&#x27;s an inexpensive intervention that reduces compute cost and lets us train better models. You could argue that not much is new here, but lots of great ideas seem obvious in retrospect – like &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1512.03385&quot;&gt;ResNets&lt;&#x2F;a&gt; adding the residual connection for the first time.&lt;&#x2F;p&gt;
&lt;p&gt;One point to be mindful of when making this kind of intervention is whether it negatively affects downstream behaviour. I expect that MoDE had been used internally at GDM for a while before this paper was released, &amp;amp; so I expect to see analyses in this direction soon.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>What sorts of systems can be deceptive?</title>
		<published>2022-10-31T00:00:00+00:00</published>
		<updated>2022-10-31T00:00:00+00:00</updated>
		<link rel="alternate" type="text/html" href="https://inwaves.github.io/posts/deceptive-systems/"/>
		<id>https://inwaves.github.io/posts/deceptive-systems/</id>
    
		<content type="html" xml:base="https://inwaves.github.io/posts/deceptive-systems/">&lt;p&gt;&lt;em&gt;The following is a crosspost from &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.alignmentforum.org&#x2F;posts&#x2F;atSHHCSP3NKBtqxes&#x2F;what-sorts-of-systems-can-be-deceptive&quot;&gt;the alignment forum&lt;&#x2F;a&gt;. This work was done as part of SERI MATS, under Leo Gao&#x27;s guidance. Thank you to Erik Jenner and Johannes Treutlein for discussions and comments on the draft.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m interested in understanding deception in machine intelligence better. Specifically, I want to understand what precursors there are to deceptive alignment, and whether upon observing these precursors, we can change our approach to achieve better outcomes. In this article, I outline my current thinking on this topic, and consider a bunch of properties that systems which can be deceptive might share. I am still pretty confused about how this works, and I don&#x27;t yet have good ideas for what comes next.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;a class=&quot;zola-anchor&quot; href=&quot;#preliminaries&quot; aria-label=&quot;Anchor link for: preliminaries&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;h2 id=&quot;what-is-deception&quot;&gt;What is deception?&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-is-deception&quot; aria-label=&quot;Anchor link for: what-is-deception&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;p&gt;Commonly, by deception we mean a scenario where an intelligent system behaves in a way that hides what information it knows from another system, or from a human overseer. Fundamentally, the reason to be deceptive is that behaving honestly would lead to the system being penalised by its designers, perhaps through a training process that can intervene directly on its internal mechanisms.&lt;&#x2F;p&gt;
&lt;p&gt;In the context of &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1906.01820&quot;&gt;Risks from Learned Optimization&lt;&#x2F;a&gt;, deceptive alignment occurs when a system internally has a goal other than what we would like it to be. If this system is aware that it might be shut down or altered if it revealed this discrepancy, it is incentivised to play along, i.e. behave as though it is optimising the humans&#x27; goal. Once the training process is complete and the system is safe to pursue its own goal, it does so without repercussions.&lt;&#x2F;p&gt;
&lt;p&gt;As described in the paper, deception is an inner misalignment failure, i.e. in the system there exists an inner optimiser whose goals may be different to the base optimiser&#x27;s goals. If this is the case, even if we select for models which appear to seek our base objective, the objective they actually end up pursuing may be different. These systems are deceptively aligned.&lt;&#x2F;p&gt;
&lt;p&gt;In this framing, the main way we might get deceptive alignment is through a system that performs internal search. While search is a central example, I&#x27;m curious to understand more generally what the prerequisites are for deceptive alignment. I&#x27;m particularly interested in the connection with consequentialism. In a sense, deception is just one of the many strategies a consequentialist agent might use to achieve its goals.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;how-does-the-idea-of-optimisation-matter-here&quot;&gt;How does the idea of optimisation matter here?&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-does-the-idea-of-optimisation-matter-here&quot; aria-label=&quot;Anchor link for: how-does-the-idea-of-optimisation-matter-here&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;p&gt;In this article, I use &quot;optimisation&quot; in the context of an &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;znfkdCoHMANwqc2WE&#x2F;the-ground-of-optimization-1&quot;&gt;optimising system&lt;&#x2F;a&gt; – a system which tends to reconfigure a space toward a small set of target states, while being robust to perturbation. This does not require the existence of a part of the system which is doing the optimisation, i.e. a separate optimiser. In this view, I think it&#x27;s possible to have a continuum of optimising systems, ranging from search over possible actions on one extreme to a set of heuristics on the other, such that all these systems outwardly behave as consequentialists, regardless of how they&#x27;re implemented.&lt;&#x2F;p&gt;
&lt;p&gt;This is related to the view that some folks at MIRI have that consequentialism is about the work that&#x27;s being done, rather than the type of system that is carrying it out. If that is the case, then in the limit of generalisation capability (i.e. systems which exhibit robust generalisation), it doesn&#x27;t matter how the system is implemented – specifically, whether it&#x27;s doing search or not – for how likely it is to exhibit deceptive behaviour. Note that my impression of Eliezer&#x27;s view on this is that he does think that you can build a weaker system which doesn&#x27;t exhibit some undesirable property such as deception, but that this weaker system is just not enough to carry out a pivotal act. (I base this mostly on &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.lesswrong.com&#x2F;s&#x2F;n945eovrA3oDueqtq&#x2F;p&#x2F;7im8at9PmhbT4JHsW&quot;&gt;Eliezer&#x27;s and Richard&#x27;s conversation&lt;&#x2F;a&gt;.)&lt;&#x2F;p&gt;
&lt;h1 id=&quot;what-properties-are-upstream-of-deception&quot;&gt;What properties are upstream of deception?&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-properties-are-upstream-of-deception&quot; aria-label=&quot;Anchor link for: what-properties-are-upstream-of-deception&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;p&gt;Let&#x27;s assume that some system has a consequentialist objective. This system lies somewhere on an axis of generalisation capability. On one extreme of this axis are &lt;strong&gt;controllers&lt;&#x2F;strong&gt;, systems which can be arbitrarily capable at carrying out their intended purpose, but which cannot extrapolate to other tasks. By &quot;arbitrarily capable&quot; I mean to hint that controllers can be very complex, i.e. they may have to carry out quite difficult tasks, by our standards, while still not being able to generalise to other tasks. One exception might be if those tasks are subtasks of their initial task, in the same way &quot;boil water&quot; is a subtask of a robot that makes coffee.&lt;&#x2F;p&gt;
&lt;p&gt;In my model, controllers don&#x27;t &quot;want&quot; anything, not even with respect to accomplishing the task they have. Thermostats don&#x27;t want to regulate temperature; they just have a set of rules and heuristics that apply in particular scenarios. If the thermostat has a rule: &quot;if the temperature is under 20C, turn on the heating&quot;, there&#x27;s no sense in which it &quot;wants&quot; to turn on the heating when the room is cold; it just does so. If this rule were removed and there were no other rules looking at the same scenario, the thermostat wouldn&#x27;t try to find a way around the removal and still turn the heating on; it would not even know it needed to. It seems unlikely that controllers would be deceptively aligned.&lt;&#x2F;p&gt;
&lt;p&gt;On the other extreme of the axis are optimisers. These are systems which can generalise to states that were not encountered during the training process. In particular, they can generalise an objective of the type &quot;do whatever it takes to achieve X&quot; from states seen during training to states encountered during deployment. These systems can be deceptively aligned.&lt;&#x2F;p&gt;
&lt;p&gt;We kind of understand what systems look like in these extremes, but it&#x27;s unclear to me what characteristics intermediate systems – those which are some mix of searching and heuristic behaviour – have. It seems important to find this out, given that neural networks seem to be somewhere on this continuum. If scaling deep learning leads to systems which generalise better – and thus are more like optimisers – then we&#x27;ll hit a point where deceptive alignment is &quot;natural&quot;.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;disentangling-generalisation-and-consequentialism&quot;&gt;Disentangling generalisation and consequentialism&lt;a class=&quot;zola-anchor&quot; href=&quot;#disentangling-generalisation-and-consequentialism&quot; aria-label=&quot;Anchor link for: disentangling-generalisation-and-consequentialism&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;p&gt;Here&#x27;s an idea: to have systems which are deceptively aligned, you need them to both have a consequentialist objective and have high generalisation capability.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ll continue with the same axis I described in the previous section. On one end are low-generalisation systems like look-up tables, expert systems and bags of heuristics. On the other, are searchy, optimiser-like systems, which generalise well to unseen states. Another way to think about this is: on one extreme, the outer training loop does the heavy lifting. All strategies are learned during the training process, and there is no mechanism by which a system can learn after deployment. On the other, the inner loop does most of the work; it finds new strategies at runtime, probably using search. Things in the middle use a combination of memorisation of data, rules, heuristics learned during training, as well as some degree of in-context&#x2F;online learning. GPT seems to be somewhere between lookup tables and AIXI, probably closer to the former.&lt;&#x2F;p&gt;
&lt;p&gt;To see how deception arises in my model, I want to introduce a new axis. On this axis, systems range from &quot;has a non-consequentialist objective&quot; to &quot;has a consequentialist objective&quot;. Here, the objective is the objective of the system. In the case of a controller, it&#x27;s a behavioural objective, in the sense that we could accurately model the controller&#x27;s behaviour as &quot;trying to achieve X&quot;. For optimisers, the objective refers to their mesa-objective. This coordinate system also implies a &quot;base objective&quot; axis – this is not the same as the y-axis in the figure.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;deceptive_systems_chart.png&quot; alt=&quot;2D coordinate system showing generalisation capability vs consequentialism&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;A 2D system of coordinates: generalisation capability against consequentialism in the behavioural objective.&lt;&#x2F;p&gt;
&lt;p&gt;I want to examine four cases, corresponding to four imaginary quadrants (I don&#x27;t think there will be a meaningful boundary, nor do I think that the quadrants are drawn realistically, i.e. that GPT is LGNC. I&#x27;m using this for the sake of analysis):&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Low-generalisation, non-consequentialist objective&lt;&#x2F;strong&gt; &lt;strong&gt;(LGNC)&lt;&#x2F;strong&gt; systems: designed to achieve a task that is relatively simple, and which cannot extrapolate past that task. For example, a trivia system that memorises and retrieves facts about the world, without modelling agents.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Low-generalisation, consequentialist objective&lt;&#x2F;strong&gt; &lt;strong&gt;(LGC)&lt;&#x2F;strong&gt; systems: designed to achieve a complex task that requires completing lots of smaller steps, and potentially figuring out what those steps are. An example here is a tabular policy learned using RL.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;High-generalisation, non-consequentialist objective (HGNC)&lt;&#x2F;strong&gt; systems: searchy things which accomplish some relatively simple task, like retrieving a fact about the world without modelling agents.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;High-generalisation, consequentialist objective&lt;&#x2F;strong&gt; &lt;strong&gt;(HGC)&lt;&#x2F;strong&gt; systems: searchy things with complex goals which require planning and achieving other subtasks, not being turned off or modified, the usual.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;I&#x27;m going to make a bunch of claims regarding these four quadrants&#x2F;regimes, all of which are intuitions rather than hypotheses which I currently have lots of evidence for. Here goes:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;LGNC systems cannot be deceptive except through Goodharting on the task, as discussed &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;8whGos5JCdBzDbZhH&#x2F;framings-of-deceptive-alignment&quot;&gt;here&lt;&#x2F;a&gt;. This is because these systems can&#x27;t generalise beyond what they were trained on, and cannot discover deceptive strategies at runtime, through search. For these systems, their capability comes largely from the training process. LGNC systems are safe but useless.&lt;&#x2F;li&gt;
&lt;li&gt;Similarly, LGC can only carry out deception if they observed it in the training data. Because they have low generalisation capability, they can&#x27;t extrapolate or recompose it from examples which did not show deception.&lt;&#x2F;li&gt;
&lt;li&gt;HGNC systems can be deceptive, but they are somewhat less dangerous than their consequentialist counterparts. These approximately map onto tool AIs. It seems like these aren&#x27;t a central problem, as there are &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.gwern.net&#x2F;Tool-AI&quot;&gt;economic incentives&lt;&#x2F;a&gt; for HGC systems to develop rather than HGNC systems.&lt;&#x2F;li&gt;
&lt;li&gt;Deception &lt;em&gt;does arise&lt;&#x2F;em&gt; when moving from HGNC to HGC systems (acquiring consequentialism – &quot;motive&quot;) or from LGC to HGC systems (acquiring generalisation power&#x2F;search capabilities – &quot;means&quot;). These systems are very likely to be deceptive, if deception turns out to be a useful strategy for achieving their goals. These systems behave coherently and can be described as maximising some utility function.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Insofar as these two axes are separate, I imagine that there&#x27;s something like a Pareto front in the space of possible models, such that we can get most of our capability for the least amount of risk of deception possible. This is the way in which I see further work in this area to be impactful.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;where-this-falls-short&quot;&gt;Where this falls short&lt;a class=&quot;zola-anchor&quot; href=&quot;#where-this-falls-short&quot; aria-label=&quot;Anchor link for: where-this-falls-short&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;p&gt;There are various things here that confuse me, the main being what the x-axis represents. There is something unsatisfying about saying that deception happens because of generalisation capability and calling it a day. I&#x27;m interested if we can pinpoint deception even more precisely, in terms of properties like Ajeya&#x27;s &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;pRkFkzwKZ2zfa3R6H&#x2F;without-specific-countermeasures-the-easiest-path-to&quot;&gt;situational awareness&lt;&#x2F;a&gt; (&quot;playing the training game&quot;), or goal-directedness. In other words, what mechanisms enable deceptive alignment, and where do they emerge as a system becomes more capable?&lt;&#x2F;p&gt;
&lt;p&gt;Another confusion regarding this 2D system idea is that the two axes are not orthogonal. In my view, it&#x27;s not possible to become more capable and not become consequentialist at all. This is because systems become more capable as they are trained on scenarios that are more varied, more broad, and more complex. That gets you consequentialism. Still, even if the axes are correlated, it doesn&#x27;t mean that all systems lie on the first diagonal, and for every increase in capability you get a proportional increase in consequentialism.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;appendix-degree-of-coupling&quot;&gt;Appendix: Degree of coupling?&lt;a class=&quot;zola-anchor&quot; href=&quot;#appendix-degree-of-coupling&quot; aria-label=&quot;Anchor link for: appendix-degree-of-coupling&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;p&gt;It seems like an axis which overlaps to a significant degree with generalisation ability is the degree of internal coupling of the system. On the low-coupling extreme, each of a system&#x27;s components is practically independent from the others. Their effects can chain together, such that after component A fulfils its purpose, component B can activate if its conditions are met. But there are no situations where in order for the system to achieve some behaviour two components need to fire at the same time, or be in two particular states, or interact in a specific way.&lt;&#x2F;p&gt;
&lt;p&gt;Moving toward the right on this axis, we get systems which are more coupled. There are behaviours which require more complex interactions between components, and these incentivise a sort of internal restructuring. For example, it&#x27;s likely that components are reused to produce distinct behaviours: A + X = B1, A + Y = B2. I think it&#x27;s the case that some components become very general, and so are used more often. Others remain specialised, and are only seldom used.&lt;&#x2F;p&gt;
&lt;p&gt;In my model, the further along this axis a system is, the more it relies on general components rather than case-specific heuristics and rules. A fully coupled system is one which uses only a handful of components which always interact in complex ways. In an extreme case, a system may use just one component – for example an exhaustive search over world states which maximise a utility function.&lt;&#x2F;p&gt;
&lt;p&gt;If coupling is something we observe in more capable systems, it might have implications for e.g. how useful our interpretability techniques are for detecting particular types of cognition.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Why you might expect homogeneous takeoff: evidence from ML research</title>
		<published>2022-07-17T00:00:00+00:00</published>
		<updated>2022-07-17T00:00:00+00:00</updated>
		<link rel="alternate" type="text/html" href="https://inwaves.github.io/posts/homogeneous-takeoff/"/>
		<id>https://inwaves.github.io/posts/homogeneous-takeoff/</id>
    
		<content type="html" xml:base="https://inwaves.github.io/posts/homogeneous-takeoff/">&lt;p&gt;&lt;em&gt;The below is a cross-post from &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.alignmentforum.org&#x2F;posts&#x2F;RQn45KzN5cojLLb3L&#x2F;why-you-might-expect-homogeneous-take-off-evidence-from-ml&quot;&gt;the alignment forum&lt;&#x2F;a&gt;.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;a class=&quot;zola-anchor&quot; href=&quot;#introduction&quot; aria-label=&quot;Anchor link for: introduction&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;p&gt;This article aims to draw a connection between recent ML research and the claim that future advanced AI systems may be homogenous. First, I briefly review &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.alignmentforum.org&#x2F;posts&#x2F;mKBfa8v4S9pNKSyKK&#x2F;homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios&quot;&gt;this article&lt;&#x2F;a&gt;, where the idea of homogenous take-off is introduced. Then, I outline two different arguments why you might update in the direction of homogenous take-off. For each of the arguments I mention key uncertainties that I have about the argument itself, as well as broader open questions.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;tl-dr&quot;&gt;TL; DR&lt;a class=&quot;zola-anchor&quot; href=&quot;#tl-dr&quot; aria-label=&quot;Anchor link for: tl-dr&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;p&gt;I present two reasons to believe that as models become larger they also become more homogenous, i.e. they behave more similarly to each other:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Variance between models behaves unimodally in the overparameterised regime: it peaks around the interpolation threshold, then decreases monotonically. Decreased variance means that models make similar predictions across different training runs (captured as variance from initialisation) and different sampling of the training data (variance from sampling);&lt;&#x2F;li&gt;
&lt;li&gt;Neural networks have a strong simplicity bias even before training, which might mean that multiple training runs with different hyperparameters, initialisation schemes etc. result in essentially the same model.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I&#x27;ve somewhat updated in the direction of homogenous take-off as a result of these arguments, though I think that there are still ways in which it&#x27;s unclear if e.g. decreasing variance with size rules out heterogeneity.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;what-s-homogeneous-take-off&quot;&gt;What&#x27;s homogeneous take-off?&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-s-homogeneous-take-off&quot; aria-label=&quot;Anchor link for: what-s-homogeneous-take-off&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;p&gt;There are several axes along which different AI takeoff scenarios could differ: speed, continuity, and number of main actors. &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.alignmentforum.org&#x2F;posts&#x2F;mKBfa8v4S9pNKSyKK&#x2F;homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios&quot;&gt;Homogeneity vs. heterogeneity in AI takeoff scenarios&lt;&#x2F;a&gt; introduces a new way to look at a potential take-off, through the lens of model &lt;strong&gt;homogeneity&lt;&#x2F;strong&gt;. Homogeneity intuitively refers to how similar models are at any given time given some definition of similarity. We might specifically refer to homogeneity with regards to alignment, which, again intuitively, means &quot;models are more or less aligned to the same degree&quot; (or: &quot;aligned models will not coexist with unaligned models&quot;).&lt;&#x2F;p&gt;
&lt;p&gt;More formally we mean something like models having similar properties, e.g. alignment properties. In my mind, an alignment property might be something like &quot;corrigibility&quot; or &quot;truthfulness&quot;, though it&#x27;s unclear to me to what extent two models which are, say, truthful, are also homogenous. I think working toward a clearer, more precise definition of homogeneity is probably useful in determining what actually counts as evidence for homogenous systems being more likely, though I don&#x27;t try to do so in this write-up.&lt;&#x2F;p&gt;
&lt;p&gt;The article sets out a list of arguments supporting the idea of homogenous take-off, which I parse as &quot;evidence from the economics of large scale machine learning&quot;. Without going into too much detail – I recommend reading the original article for the full arguments –, these are:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Training a model is more expensive than running it&lt;&#x2F;strong&gt;. This is a relatively straightforward claim which extrapolates from the landscape we have today, where some large language models reportedly have had training budgets in the millions of US dollars, with comparatively little cost to run inference&#x2F;serve the models themselves once trained.&lt;&#x2F;li&gt;
&lt;li&gt;**Training models from scratch is not competitive once the first advanced system is released. **To me this follows from 1., in the sense that if it is economically useful to deploy more than one model simultaneously, it&#x27;s likely that the additional models will be copies of the original (perhaps fine-tuned on different tasks) rather than new models trained from scratch.&lt;&#x2F;li&gt;
&lt;li&gt;**Copying is more competitive than (untested) alternative approaches. **Here I think it&#x27;s worth disentangling two ways of copying:
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Direct copying&lt;&#x2F;strong&gt; of the first advanced system, possibly by using the same weights, or by running the exact same training process. There are reasons to believe that direct copying might not be possible or even desirable, since e.g. states might not want to use a competing state&#x27;s advanced system.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Indirect copying&lt;&#x2F;strong&gt; is using the same techniques as the creators of the original system, but not identical training runs&#x2F;hyperparameters. This scenario seems more likely to me, and it&#x27;s here where the arguments I present on variance&#x2F;simplicity bias are most important, since they show that different runs may not necessarily result in different models.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;**Homogeneity is preserved during initial takeoff. **Here the argument is that later generations of AIs will also be homogenous, at least with respect to alignment. This is because we either use the first generation systems to align the next generations, or we use ~the same techniques we used on the 1st generation to align the next generations. The idea is that both approaches result in systems with the same alignment properties. It&#x27;s unclear to me whether the same kind of argument holds for something other than alignment properties – and if not, why not.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;In this article I want to present two technical arguments from recent ML research that support the idea of homogenous take-off. First, as models become larger, variance between models decreases. Second, neural networks seem to be biased toward simple solutions even before training.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;argument-from-variance&quot;&gt;Argument from variance&lt;a class=&quot;zola-anchor&quot; href=&quot;#argument-from-variance&quot; aria-label=&quot;Anchor link for: argument-from-variance&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;h2 id=&quot;bias-variance-decomposition&quot;&gt;Bias-variance decomposition&lt;a class=&quot;zola-anchor&quot; href=&quot;#bias-variance-decomposition&quot; aria-label=&quot;Anchor link for: bias-variance-decomposition&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;p&gt;One of the main practical insights of statistical learning theory is related to the bias-variance decomposition of mean squared error. In this section I&#x27;ll be introducing the concepts of bias and variance and discussing their importance in the classical and the overparameterised regimes. I&#x27;ll be using the notation from &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper&#x2F;2020&#x2F;file&#x2F;7d420e2b2939762031eed0447a9be19f-Paper.pdf&quot;&gt;Adlam &amp;amp; Pennington, 2020&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The main idea is that for a supervised learning task where a model ($$\hat{y}$$) is minimising mean squared error on a training set $$\mathcal{D}&lt;em&gt;{tr}$$, we can decompose the on a test point $$x \in \mathcal{D}&lt;&#x2F;em&gt;{te}$$  as:&lt;&#x2F;p&gt;
&lt;p&gt;$$[\mathbb{E}[\hat y(x) - y(x)]^2 = (\mathbb{E} \hat y(x) - \mathbb{E} y(x))^2 + \mathbb{V}[\hat y(x)] + \mathbb{V}[y(x)]]$$&lt;&#x2F;p&gt;
&lt;p&gt;where \(y(x)\) is the ground truth. The first term is the squared bias, the second is the variance and the third is irreducible noise in the test data. The randomness in these variables is canonically taken to come from sampling noise, though as we&#x27;ll see shortly there are other sources too.&lt;&#x2F;p&gt;
&lt;p&gt;Looking at the terms more closely, bias is how much the average prediction – across models trained on different realisations of the training set – differs from the ground truth. Bias is typically interpreted as error resulting from incorrect assumptions about the data, a phenomenon otherwise known as underfitting. For example, trying to interpolate a polynomial function with a linear model leads to high bias.&lt;&#x2F;p&gt;
&lt;p&gt;The second term, variance, refers to how much each individual model&#x27;s prediction differs from the average prediction, and has historically been tied to overfitting to noise. If models&#x27; predictions on the test set vary widely, it&#x27;s because they&#x27;ve all learned spurious correlations in their specific training set which do not hold on test data. The last term is irreducible noise in the test set.&lt;&#x2F;p&gt;
&lt;p&gt;Ideally, we&#x27;d like both bias and variance to be small. However, in practice, bias and variance seem to trade off against each other as model capacity is increased, resulting in the typical U-shaped curve for test risk in Figure 1a. This trade-off implies that to achieve the optimal test risk, a model should aim for the &quot;sweet spot&quot; where the trade-off is optimal. If models are larger than the optimal size, variance increases, and the result is overfitting to the training data and poor generalisation performance on test data. If they are smaller, they underfit the training data and do relatively poorly on all datasets.&lt;&#x2F;p&gt;
&lt;p&gt;This is the received wisdom from the bias-variance decomposition in the &quot;classical&quot; regime, and the dominating view pre-deep learning. This is mostly invalidated by deep neural networks, which generalise well despite being very large relative to the datasets they are trained on. The phenomenon of double descent (see e.g. &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.pnas.org&#x2F;doi&#x2F;full&#x2F;10.1073&#x2F;pnas.1903070116&quot;&gt;Belkin et al., 2019&lt;&#x2F;a&gt;) illustrates the capacity of large models to interpolate (i.e. perfectly fit) the data, and yet perform well to unseen data. In Figure 1b, as model size is increased, we move from the classical U-shaped risk curve to a peak at the interpolation threshold, with risk decreasing monotonically past the peak to values that are below the previous minimum.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;adlam_pennington2020.jpeg&quot; alt=&quot;Figure 1&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;**Figure 1. From **&lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.pnas.org&#x2F;doi&#x2F;full&#x2F;10.1073&#x2F;pnas.1903070116&quot;&gt;&lt;strong&gt;Belkin et al., 2019&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;. A visual representation of (a) the underparameterised regime, where the bias-variance trade-off occurs as models increase in capacity (here formalised as the size of a hypothesis class \(\mathcal{H}\)) and (b) the modern overparameterised regime, where test risk decreases despite models which are large relative to the size of their training datasets.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;double-descent-and-variance&quot;&gt;Double descent and variance&lt;a class=&quot;zola-anchor&quot; href=&quot;#double-descent-and-variance&quot; aria-label=&quot;Anchor link for: double-descent-and-variance&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;p&gt;Several recent papers (&lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;proceedings.mlr.press&#x2F;v119&#x2F;yang20j&#x2F;yang20j.pdf&quot;&gt;Yang et al., 2020&lt;&#x2F;a&gt;, &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.jmlr.org&#x2F;papers&#x2F;volume22&#x2F;20-1211&#x2F;20-1211.pdf&quot;&gt;Lin &amp;amp; Dobriban, 2021&lt;&#x2F;a&gt;, &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper&#x2F;2020&#x2F;file&#x2F;7d420e2b2939762031eed0447a9be19f-Paper.pdf&quot;&gt;Adlam &amp;amp; Pennington, 2020&lt;&#x2F;a&gt;) examine double descent through the lens of the bias-variance decomposition. Broadly speaking, the main finding is that variance behaves unimodally – it increases, peaks, and then decreases monotonically. Depending on the magnitude of bias relative to variance, several test risk curves can be obtained, including double descent – see Figure 2. below.&lt;&#x2F;p&gt;
&lt;p&gt;The important observation here is that as models increase in size, variance decreases. To zoom out a bit, remember that variance captures the degree to which models differ in their predictions across different training runs. Decreasing variance means that models become more homogenous. In a sense, this follows directly from double descent, since we know that bias decreases with size and that after the interpolation threshold test risk decreases monotonically.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;yang2020.jpeg&quot; alt=&quot;Figure 2&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2. From &lt;strong&gt;&lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;proceedings.mlr.press&#x2F;v119&#x2F;yang20j&#x2F;yang20j.pdf&quot;&gt;&lt;strong&gt;Yang et al., 2020&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;&lt;&#x2F;strong&gt;.&lt;&#x2F;strong&gt; A hypothetical test risk curve plotted against model complexity, alongside its bias and variance components. The three cases are as follows: (a) if bias dominates variance over the entire x-axis, then test risk follows a monotonic decrease; (b) if bias and variance dominate in different regimes, the test risk follows a double descent curve; (c) if variance dominates bias over the entire x-axis, then test risk is simply unimodal – without the initial decrease.&lt;&#x2F;p&gt;
&lt;p&gt;To try and understand what is happening with double descent, the latter two papers focus on decomposing variance into additional sources of randomness (training data sampling, parameter initialisation and label noise) and find that some components behave unimodally, while others increase up to the interpolation threshold and stay constant afterward (e.g. variance due to sampling and due to label noise, see Figure 3j).&lt;&#x2F;p&gt;
&lt;p&gt;There seems to not be any consensus on how to additionally decompose variance (including the order in which to condition on these sources of randomness – because conditioning isn&#x27;t symmetrical). Because of this, e.g. &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;link.aps.org&#x2F;pdf&#x2F;10.1103&#x2F;PhysRevResearch.4.013201&quot;&gt;Rocks &amp;amp; Mehta, 2022&lt;&#x2F;a&gt; hint that some studies are led to incorrect conclusions about the relationship between variance and double descent.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;belkin2019.jpeg&quot; alt=&quot;Figure 3&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3. From &lt;strong&gt;&lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper&#x2F;2020&#x2F;file&#x2F;7d420e2b2939762031eed0447a9be19f-Paper.pdf&quot;&gt;&lt;strong&gt;Adlam &amp;amp; Pennington, 2020&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;&lt;&#x2F;strong&gt;.&lt;&#x2F;strong&gt; Different decompositions of the bias (\(B\)) and the variance (\(V\)) of a neural network with hidden layer size \(n_1\) and dataset size \(m\). Most useful is the right-most decomposition into variance from sampling (\(V_X\)), variance from initialisation (\(V_P\)) and variance from label noise (\(V_\epsilon\)), along with their interaction effects, e.g. \(V_{PX}, V_{PX\epsilon}\). In figure (j) \(B, V_X\) and \(V_{X\epsilon}\) converge to a constant value after the interpolation threshold, and their values are no longer sensitive to increase in model size. All other sources of test risk are unimodal: they peak at the interpolation threshold and decrease with model size.&lt;&#x2F;p&gt;
&lt;p&gt;A few side-notes:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Label noise exacerbates but doesn&#x27;t cause double descent, since other components of variance peak even in the absence of label noise (Fig. 3j).&lt;&#x2F;li&gt;
&lt;li&gt;It turns out that in particular regimes the higher-level interaction effects between different sources of variance dominate the main effects, i.e. \(V_{si} &amp;gt; V_s &amp;gt; V_i\), where \(V_{s}\) is variance from sampling, \(V_i\) is variance from initialisation and \(V_{si}\) is the variance from the interaction between the two.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;It&#x27;s also worth mentioning that these studies take place in the asymptotic setting, i.e. they investigate what happens if the number of samples in the training dataset and dimensionality of the data go to infinity while maintaining a fixed ratio. &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.jmlr.org&#x2F;papers&#x2F;volume22&#x2F;20-1211&#x2F;20-1211.pdf&quot;&gt;Lin &amp;amp; Dobriban, 2021&lt;&#x2F;a&gt; find that this ratio controls the unimodal behaviour of the variance: if the ratio is below a threshold, variance peaks, then decreases; otherwise it increases.&lt;&#x2F;p&gt;
&lt;p&gt;If this analysis is correct, as long as we control the ratio we can ensure that models become more homogenous as they become larger. It&#x27;s worth noting that this hasn&#x27;t been replicated yet, to my knowledge, and that this unimodal variance explanation for double descent is not the only hypothesis, see e.g. &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper&#x2F;2021&#x2F;file&#x2F;f754186469a933256d7d64095e963594-Paper.pdf&quot;&gt;Kuzborskij et al., 2021&lt;&#x2F;a&gt; for an account of DD related to the smallest positive eigenvalue of the feature covariance matrix.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;how-might-this-turn-out-to-be-false&quot;&gt;How might this turn out to be false?&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-might-this-turn-out-to-be-false&quot; aria-label=&quot;Anchor link for: how-might-this-turn-out-to-be-false&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;p&gt;First, it&#x27;s possible that the findings from analysis of variance are not robust to changes in architecture or learning task, though at least &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;proceedings.mlr.press&#x2F;v119&#x2F;yang20j&#x2F;yang20j.pdf&quot;&gt;Yang et al. 2020&lt;&#x2F;a&gt; seem to cover quite a few experimental set-ups (including changing architecture and dataset as well as other potentially-less-impactful training hyperparameters). This means that it might be useful to do more experiments to probe the robustness of these findings. If they turn out to scale well&#x2F;hold across architectures, then this is stronger evidence in favour of homogeneity.&lt;&#x2F;p&gt;
&lt;p&gt;Second, it could be that residual variance – variance that is not eliminated through training – is enough to invalidate the homogeneity hypothesis, in the sense that residual variance could lead to different behaviour&#x2F;properties of models that exist at the same time. I&#x27;m not sure how likely this is, given that the residual variances seem to be quite small – on the order of 10^{-3} according to &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper&#x2F;2020&#x2F;file&#x2F;7d420e2b2939762031eed0447a9be19f-Paper.pdf&quot;&gt;Adlam &amp;amp; Pennington, 2020&lt;&#x2F;a&gt; – though of course here the threshold is unknown. (How much variance implies heterogeneity doesn&#x27;t seem to be a well-posed question.)&lt;&#x2F;p&gt;
&lt;p&gt;I don&#x27;t have a good idea for how to resolve this uncertainty. It seems to me that unless we can find a more precise definition of homogeneity, we can&#x27;t say exactly how much residual variance matters.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;things-i-don-t-yet-understand&quot;&gt;Things I don&#x27;t yet understand&lt;a class=&quot;zola-anchor&quot; href=&quot;#things-i-don-t-yet-understand&quot; aria-label=&quot;Anchor link for: things-i-don-t-yet-understand&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;How does the fixed-design&#x2F;random-design decomposition affect the result? For example see &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1903.08560&quot;&gt;Hastie et al., 2022&lt;&#x2F;a&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Lots of these experiments use random features, and it&#x27;s unclear to me why this is more appropriate&#x2F;easy to analyse than shallow neural networks, which presumably are closer to what we care about.&lt;&#x2F;li&gt;
&lt;li&gt;Where does the variance from optimisation fit in? Is it the same as variance from initialisation, which is where the optimiser starts? E.g. &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1810.08591.pdf&quot;&gt;Neal et al., 2018&lt;&#x2F;a&gt; mention variance due to optimisation, but they don&#x27;t study how bias and variance change during training.
&lt;ul&gt;
&lt;li&gt;They point to variance from optimisation as encompassing: random initialisation and stochastic mini-batching, but they also say that their results hold even with batch gradient descent.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;open-questions&quot;&gt;Open questions&lt;a class=&quot;zola-anchor&quot; href=&quot;#open-questions&quot; aria-label=&quot;Anchor link for: open-questions&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Should we expect &quot;prediction&quot; homogeneity to translate to alignment properties?&lt;&#x2F;li&gt;
&lt;li&gt;Why does variance have unimodal behaviour? It might be worth replicating the experiments in &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.jmlr.org&#x2F;papers&#x2F;volume22&#x2F;20-1211&#x2F;20-1211.pdf&quot;&gt;Lin &amp;amp; Dobriban, 2021&lt;&#x2F;a&gt; where they use the parameterisation level and data aspect ratio to control the variance.
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;proceedings.mlr.press&#x2F;v119&#x2F;yang20j&#x2F;yang20j.pdf&quot;&gt;Yang et al., 2020&lt;&#x2F;a&gt; conjecture that it&#x27;s regularisation that leads to variance decreasing past the peak, though this seems like a broad remark that does not add much useful information.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;argument-from-simplicity-bias&quot;&gt;Argument from simplicity bias&lt;a class=&quot;zola-anchor&quot; href=&quot;#argument-from-simplicity-bias&quot; aria-label=&quot;Anchor link for: argument-from-simplicity-bias&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;p&gt;We have good empirical evidence that neural networks are biased toward simple functions which fit the data. There&#x27;s no consensus on the mechanism behind this bias, but there are lots of competing explanations.&lt;&#x2F;p&gt;
&lt;p&gt;One recent such explanation is that their parameter-function maps are biased toward low-complexity functions; that is, even before training NN architectures induce a strong preference for simplicity. See &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;YSFJosoHYFyXjoYWa&#x2F;why-neural-networks-generalise-and-why-they-are-kind-of&quot;&gt;this LW article&lt;&#x2F;a&gt; for a jumping-off point, or go directly to the technical papers: &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1805.08522&quot;&gt;Valle-Perez, Camargo &amp;amp; Louis, 2018&lt;&#x2F;a&gt;, &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1909.11522&quot;&gt;Mingard et al., 2019&lt;&#x2F;a&gt;, &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.jmlr.org&#x2F;papers&#x2F;volume22&#x2F;20-676&#x2F;20-676.pdf&quot;&gt;Mingard et al., 2021&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;If this analysis is correct, then various proposed mechanisms for why DNNs generalise which are related to optimiser choice or hyperparameter tuning are only small (or &quot;second-order&quot;) deviations from the posterior \(P_B(f|S)\) whose bias is essentially induced by the prior \(P(f)\).&lt;&#x2F;p&gt;
&lt;p&gt;This might lead you to believe that given a fixed architecture, different initialisation schemes, optimisers, hyperparameters etc. do not contribute substantially to the properties that the trained system has, or, differently put, that different experimental setups do not result in systems which differ in the ways we care about.&lt;&#x2F;p&gt;
&lt;p&gt;This is consistent with the finding that variance decreases with scale, at least if we interpret the findings in Section 5 of &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1909.11522&quot;&gt;Mingard et al., 2019&lt;&#x2F;a&gt; to mean that adding more layers results in stronger bias toward simple functions. I&#x27;d be excited about work that directly connects these two insights, especially since we don&#x27;t necessarily know yet why variance is unimodal.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m a bit more sceptical that this line of argument supports homogeneity directly, mostly because I don&#x27;t think that a biased parameter-function map explains all the properties of models found through e.g. SGD (nor do I think the Mingard et al. papers make these claims). If the influence of specific training hyperparameters is enough to induce heterogeneity between runs – again, only gesturing at the concept of heterogeneity rather than defining it – then even if the parameter-function map hypothesis of generalisation is true, it&#x27;s evidence &lt;em&gt;against&lt;&#x2F;em&gt; homogeneity.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;how-might-this-turn-out-to-be-false-1&quot;&gt;How might this turn out to be false?&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-might-this-turn-out-to-be-false-1&quot; aria-label=&quot;Anchor link for: how-might-this-turn-out-to-be-false-1&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The biased prior drives most of the inductive bias, but it doesn&#x27;t explain everything.&lt;&#x2F;strong&gt; Even if details about the training setup do not account for most of DNNs&#x27; capacity to generalise, they may still account for some particular property which is relevant from an alignment perspective;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Simplicity is not the same as homogeneity.&lt;&#x2F;strong&gt; Even if all functions that NNs tend to find are simple by some measure, it doesn&#x27;t mean that they are the same function. There might be some key input where two up-to-then identical functions diverge, which could lead to negative outcomes. Again, it&#x27;s possible that we won&#x27;t be able to prove that two such functions are the same.&lt;&#x2F;li&gt;
&lt;li&gt;**Objections to the biased prior hypothesis. **It could be that the biased parameter-function map account of NN generalisation does not scale to larger networks&#x2F;more complex architectures&#x2F;other tasks (for some discussion, see &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;5p4ynEJQ8nXxp2sxC&#x2F;parsing-chris-mingard-on-neural-networks#Scalability&quot;&gt;this article&lt;&#x2F;a&gt;). This might mean that some other hypothesis better explains NNs&#x27; performance in the overparameterised regime – there are many related to: the stochasticity of gradient descent, the loss landscape (through basins of attraction or through flat minima), NNs&#x27; similarity to GPs, implicit regularisation and others – which might lead us to update away from model homogeneity.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;a class=&quot;zola-anchor&quot; href=&quot;#summary&quot; aria-label=&quot;Anchor link for: summary&quot; style=&quot;visibility: hidden;&quot;&gt;&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;p&gt;This article outlines two arguments from recent ML research for why homogenous take-off is a plausible story. One stems from an empirically observed decrease in variance with model size, which is consistent with the double descent phenomenon. The other is a consequence of the finding that neural networks are a priori biased toward simple functions, which means that they are likely to find solutions with similar properties regardless of the particular training parameters.&lt;&#x2F;p&gt;
&lt;p&gt;I think there&#x27;s still work to be done on both of these arguments, and I&#x27;d be much more willing to update in favour of homogenous takeoff if the findings were more robust or we had a better understanding of e.g. why variance is unimodal. But it seems worthwhile to make this connection and get more people thinking and talking about the likelihood of homogeneity in take-off scenarios.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Inductive bias of neural networks on 1D regression: an empirical examination</title>
		<published>2022-06-01T00:00:00+00:00</published>
		<updated>2022-06-01T00:00:00+00:00</updated>
		<link rel="alternate" type="text/html" href="https://inwaves.github.io/posts/inductive-bias/"/>
		<id>https://inwaves.github.io/posts/inductive-bias/</id>
    
		<content type="html" xml:base="https://inwaves.github.io/posts/inductive-bias/">&lt;p&gt;The full write-up of my dissertation is now public &lt;a href=&quot;&#x2F;assets&#x2F;inductive-bias-of-nn.pdf&quot;&gt;here&lt;&#x2F;a&gt;. You can also find the code on &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;inwaves&#x2F;nn-inductive-bias-regression&quot;&gt;Github&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Here is the abstract:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Modern network architectures generalise well even when the size of the network is very large relative to the amount of data it is trained on. This contradicts the received wisdom from statistical learning theory that models with high capacity overfit training data and do poorly on test data. One explanation for why neural networks generalise so well comes in the form of an implicit bias of the optimisation process. Recent theoretical results pinpoint the bias of gradient descent optimisation toward a class of smooth functions called interpolating splines. In this paper, we conduct a large-scale empirical evaluation of these results in the univariate regression case when subjected to changes in the training set-up along several hyperparameters commonly tweaked in practice. We find that these results are robust for shallow networks, but that the bias seems to change as network depth is increased. We additionally highlight several areas that could be further explored in order to better understand this bias and to generate practical recommendations for future machine learning systems.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Non-factorised identifiable variational autoencoders for causal discovery and out-of-distribution generalisation</title>
		<published>2022-03-01T00:00:00+00:00</published>
		<updated>2022-03-01T00:00:00+00:00</updated>
		<link rel="alternate" type="text/html" href="https://inwaves.github.io/posts/icarl/"/>
		<id>https://inwaves.github.io/posts/icarl/</id>
    
		<content type="html" xml:base="https://inwaves.github.io/posts/icarl/">&lt;p&gt;Abstract: &quot;In many situations, given a set of observations, we would like to find the factors which cause or influence the data we observe. To learn these factors, we can use a deep generative model such as a variational auto-encoder (VAE), which maps the data to a latent space. However, a limitation of the VAE is that it is not identifiable, in the sense that two different sets of parameters may yield the same model. To address this, it is possible to augment the VAE in such a way that it becomes identifiable while remaining capable of flexible representations. This paper explores invariant causal representation learning (ICaRL), an algorithm for causal representation learning with possible applications in causal discovery and out-of-distribution generalisation.&quot;&lt;&#x2F;p&gt;
&lt;p&gt;Full &lt;a href=&quot;&#x2F;assets&#x2F;icarl.pdf&quot;&gt;write-up&lt;&#x2F;a&gt; and &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;inwaves&#x2F;icarl&quot;&gt;code&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Are graph neural networks (GNNs) fundamentally bottlenecked?</title>
		<published>2022-02-15T00:00:00+00:00</published>
		<updated>2022-02-15T00:00:00+00:00</updated>
		<link rel="alternate" type="text/html" href="https://inwaves.github.io/posts/gnn-bottleneck/"/>
		<id>https://inwaves.github.io/posts/gnn-bottleneck/</id>
    
		<content type="html" xml:base="https://inwaves.github.io/posts/gnn-bottleneck/">&lt;p&gt;Generally speaking, the performance of neural networks on various tasks scales with their depth. Deep learning has been wildly successful on classification and regression tasks, and most recently on generating novel input. How does scale affect graph neural networks (GNN), a type of neural network designed to take advantage of structured data, where interactions can be modelled as a graph? This project discusses the phenomena of oversmoothing and oversquashing observed in deep GNNs, and evaluates the performance of a hybrid architecture. GraphTrans (introduced in &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper&#x2F;2021&#x2F;hash&#x2F;6e67691b60ed3e4a55935261314dd534-Abstract.html&quot;&gt;Wu et al. 2021&lt;&#x2F;a&gt;) is a GNN-transformer hybrid -- a GNN whose readout function is a transformer -- which has recently had promising benchmark results.&lt;&#x2F;p&gt;
&lt;p&gt;The full write-up is available at &lt;a href=&quot;&#x2F;assets&#x2F;mpnn.pdf&quot;&gt;this link&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Sharpness-aware minimisation and label noise</title>
		<published>2022-02-01T00:00:00+00:00</published>
		<updated>2022-02-01T00:00:00+00:00</updated>
		<link rel="alternate" type="text/html" href="https://inwaves.github.io/posts/sam-noisy-labels/"/>
		<id>https://inwaves.github.io/posts/sam-noisy-labels/</id>
    
		<content type="html" xml:base="https://inwaves.github.io/posts/sam-noisy-labels/">&lt;p&gt;Do flat minima generalise better? Is there a way to bias standard optimisation algorithms like stochastic gradient descent (SGD) to prefer flatter minima? Sharpness-aware minimisation (SAM), introduced by &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2010.01412&quot;&gt;Foret et al., 2020&lt;&#x2F;a&gt; is a modified version of SGD that reliably finds flat minima, resulting in improved performance. This project evaluates the performance of SAM on classification tasks where the labels are noisy -- meaning that they do not always represent the correct class.&lt;&#x2F;p&gt;
&lt;p&gt;Full &lt;a href=&quot;&#x2F;assets&#x2F;sam.pdf&quot;&gt;write-up&lt;&#x2F;a&gt; and &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;inwaves&#x2F;sam-noisy-labels&quot;&gt;code&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Comparing GPT-3 and RNNs as probabilistic generative models</title>
		<published>2022-01-01T00:00:00+00:00</published>
		<updated>2022-01-01T00:00:00+00:00</updated>
		<link rel="alternate" type="text/html" href="https://inwaves.github.io/posts/gpt-probabilistic/"/>
		<id>https://inwaves.github.io/posts/gpt-probabilistic/</id>
    
		<content type="html" xml:base="https://inwaves.github.io/posts/gpt-probabilistic/">&lt;p&gt;An investigation of transformers&#x27; and RNNs&#x27; ability to model long-term dependencies through the lens of probabilistic modelling. You can find the write-up &lt;a href=&quot;&#x2F;assets&#x2F;gpt3rnn.pdf&quot;&gt;here&lt;&#x2F;a&gt;, and the code &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;inwaves&#x2F;prob-models&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Investigating short-term climate forecasts with surrogate modelling</title>
		<published>2021-12-20T00:00:00+00:00</published>
		<updated>2021-12-20T00:00:00+00:00</updated>
		<link rel="alternate" type="text/html" href="https://inwaves.github.io/posts/climate-surrogate-model/"/>
		<id>https://inwaves.github.io/posts/climate-surrogate-model/</id>
    
		<content type="html" xml:base="https://inwaves.github.io/posts/climate-surrogate-model/">&lt;p&gt;This project uses Gaussian processes (GPs) as surrogate models for accurate climate forecasting. We emulate the UKESM1.0 climate model for precipitation, surface temperature and snow thickness. Work in collaboration with ZI Attahiru, M Salam and K Spukas.&lt;&#x2F;p&gt;
&lt;p&gt;Write-up &lt;a href=&quot;&#x2F;assets&#x2F;surrogate-modelling.pdf&quot;&gt;here&lt;&#x2F;a&gt;, codebase &lt;a rel=&quot;nofollow noreferrer&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;inwaves&#x2F;climate-surrogate-model&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Intelligent game playing with AI</title>
		<published>2018-06-01T00:00:00+00:00</published>
		<updated>2018-06-01T00:00:00+00:00</updated>
		<link rel="alternate" type="text/html" href="https://inwaves.github.io/posts/intelligent-game-playing/"/>
		<id>https://inwaves.github.io/posts/intelligent-game-playing/</id>
    
		<content type="html" xml:base="https://inwaves.github.io/posts/intelligent-game-playing/">&lt;p&gt;The full write-up of my undergraduate thesis is available &lt;a href=&quot;&#x2F;assets&#x2F;intelligent-game-playing.pdf&quot;&gt;here&lt;&#x2F;a&gt;. Here is an abstract:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Video-games contain a simulated reality not unlike ours, where the developer has control over constraints and parameters which are inaccessible in the outside world. They are generally structured around tasks that are challenging for humans, and in some cases prove to be more difficult than other activities to which our species is adapted. This makes games ideal for artificial intelligence research and benchmarking. In this project, we build an intelligent agent and test it using the General Video Game AI platform. The agent is a combination of two algorithms with an outstanding track record in decision-making and pathfinding: Monte Carlo Tree Search (MCTS) and A* search. MCTS was used by DeepMind in AlphaGo, the first intelligent agent to beat the human champion in the game of Go. A* is used widely in pathfinding problems in video-games, navigation and parsing grammars in Natural Language Processing. The hybrid agent is benchmarked against a standard implementation of MCTS.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
</content>
	</entry>
</feed>
