<!DOCTYPE html>

<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="color-scheme" content="light dark">
  
  
    
  
  <meta name="description" content="Exploring prerequisites for deceptive alignment and the relationship between generalisation capability and consequentialism">

  <title>What sorts of systems can be deceptive?</title>
  <link rel="icon" type="image/png" sizes="32x32" href="https://hd54s5ck6r3jexi5.preview.dev.igent.ai/img/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="https://hd54s5ck6r3jexi5.preview.dev.igent.ai/img/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="https://hd54s5ck6r3jexi5.preview.dev.igent.ai/img/apple-touch-icon.png">
  
  <style>

  /* light mode colors */
  body {
    --primary-color: #5871a2;
    --primary-pale-color: #5871a233;
    --primary-decoration-color: #5871a210;
    --bg-color: #ffffff;
    --text-color: #2f3030;
    --text-pale-color: #767676;
    --text-decoration-color: #a9a9a9;
    --highlight-mark-color: #5f75b020;

    --callout-note-color: #5871a2;
    --callout-tip-color: #268556;
    --callout-important-color: #885fc9;
    --callout-warning-color: #ab6632;
    --callout-caution-color: #c64e4e;
  }

  /* dark mode colors */
  body.dark {
    --primary-color: #6f8fd1;
    --primary-pale-color: #6f8fd166;
    --primary-decoration-color: #6f8fd112;
    --bg-color: #1c1c1c;
    --text-color: #c1c1c1;
    --text-pale-color: #848484;
    --text-decoration-color: #5f5f5f;
    --highlight-mark-color: #8296cb3b;

    --callout-note-color: #6f8fd1;
    --callout-tip-color: #47976f;
    --callout-important-color: #9776cd;
    --callout-warning-color: #ad7a52;
    --callout-caution-color: #d06161;
  }

  /* typography */
  body {
    --main-font: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
    --code-font: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace;
    --homepage-max-width: 768px;
    --main-max-width: 768px;
    --avatar-size: 60px;
    --font-size: 16px;
    --line-height: 1.75;
    --img-border-radius: 0px;
    --detail-border-radius: 0px;
    --dark-mode-img-brightness: 0.75;
    --dark-mode-chart-brightness: 0.75;
    --inline-code-border-radius: 2px;
    --inline-code-bg-color: var(--primary-decoration-color);
    --block-code-border-radius: 0px;
    --block-code-border-color: var(--primary-color);
    --detail-border-color: var(--primary-color);
  }

</style>

  <link rel="stylesheet" href="https://hd54s5ck6r3jexi5.preview.dev.igent.ai/main.css">
  

<link id="hl" rel="stylesheet" type="text/css" href="/hl-light.css" />



  
</head>

<body class="post">
  
  <script>
    const theme = sessionStorage.getItem('theme');
    const match = window.matchMedia("(prefers-color-scheme: dark)").matches
    if ((theme && theme == 'dark') || (!theme && match)) {
      document.body.classList.add('dark');
      const hl = document.querySelector('link#hl');
      if (hl) hl.href = 'https://hd54s5ck6r3jexi5.preview.dev.igent.ai/hl-dark.css';
    }
  </script>
  
  
<div id="wrapper">
  <div id="blank"></div>
  <aside>
    
    
    <nav>
      <ul>
        
        <li>
          <a class="h2" href="#preliminaries">Preliminaries</a>
          
          <ul>
            
            <li>
              <a class="h3" href="#what-is-deception">What is deception?</a>
            </li>
            
            <li>
              <a class="h3" href="#how-does-the-idea-of-optimisation-matter-here">How does the idea of optimisation matter here?</a>
            </li>
            
          </ul>
          
        </li>
        
        <li>
          <a class="h2" href="#what-properties-are-upstream-of-deception">What properties are upstream of deception?</a>
          
        </li>
        
        <li>
          <a class="h2" href="#disentangling-generalisation-and-consequentialism">Disentangling generalisation and consequentialism</a>
          
        </li>
        
        <li>
          <a class="h2" href="#where-this-falls-short">Where this falls short</a>
          
        </li>
        
        <li>
          <a class="h2" href="#appendix-degree-of-coupling">Appendix: Degree of coupling?</a>
          
        </li>
        
      </ul>
    </nav>
    
    
    <button id="back-to-top" aria-label="back to top">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-up"><line x1="12" y1="19" x2="12" y2="5"></line><polyline points="5 12 12 5 19 12"></polyline></svg>

    </button>
    
  </aside>
  <main>
    
<header>
  <nav>
    <a id="back-link" href="https:&#x2F;&#x2F;hd54s5ck6r3jexi5.preview.dev.igent.ai&#x2F;posts">← Back</a>
  </nav>
</header>


    <div>
      
      
      
      
      <div id="copy-cfg" style="display: none;" data-copy-icon="&lt;svg xmlns=&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&quot; viewBox=&quot;0 0 24 24&quot; width=&quot;18&quot; height=&quot;18&quot;&gt;&lt;path d=&quot;M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z&quot; fill=&quot;currentColor&quot;&gt;&lt;&#x2F;path&gt;&lt;&#x2F;svg&gt;
" data-check-icon="&lt;svg xmlns=&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&quot; viewBox=&quot;0 0 24 24&quot; width=&quot;18&quot; height=&quot;18&quot;&gt;&lt;path d=&quot;M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z&quot; fill=&quot;currentColor&quot;&gt;&lt;&#x2F;path&gt;&lt;&#x2F;svg&gt;
"></div>
      
      <article class="prose">
        <h1>What sorts of systems can be deceptive?</h1>
        <div id="post-info">
          <div id="date">
            <span id="publish">Oct 31, 2022</span>
            </div>

          
        </div>

        
        

        

        <p><em>The following is a crosspost from <a rel="nofollow noreferrer" href="https://www.alignmentforum.org/posts/atSHHCSP3NKBtqxes/what-sorts-of-systems-can-be-deceptive">the alignment forum</a>. This work was done as part of SERI MATS, under Leo Gao's guidance. Thank you to Erik Jenner and Johannes Treutlein for discussions and comments on the draft.</em></p>
<p>I'm interested in understanding deception in machine intelligence better. Specifically, I want to understand what precursors there are to deceptive alignment, and whether upon observing these precursors, we can change our approach to achieve better outcomes. In this article, I outline my current thinking on this topic, and consider a bunch of properties that systems which can be deceptive might share. I am still pretty confused about how this works, and I don't yet have good ideas for what comes next.</p>
<h1 id="preliminaries">Preliminaries<a class="zola-anchor" href="#preliminaries" aria-label="Anchor link for: preliminaries" style="visibility: hidden;"></a>
</h1>
<h2 id="what-is-deception">What is deception?<a class="zola-anchor" href="#what-is-deception" aria-label="Anchor link for: what-is-deception" style="visibility: hidden;"></a>
</h2>
<p>Commonly, by deception we mean a scenario where an intelligent system behaves in a way that hides what information it knows from another system, or from a human overseer. Fundamentally, the reason to be deceptive is that behaving honestly would lead to the system being penalised by its designers, perhaps through a training process that can intervene directly on its internal mechanisms.</p>
<p>In the context of <a rel="nofollow noreferrer" href="https://arxiv.org/abs/1906.01820">Risks from Learned Optimization</a>, deceptive alignment occurs when a system internally has a goal other than what we would like it to be. If this system is aware that it might be shut down or altered if it revealed this discrepancy, it is incentivised to play along, i.e. behave as though it is optimising the humans' goal. Once the training process is complete and the system is safe to pursue its own goal, it does so without repercussions.</p>
<p>As described in the paper, deception is an inner misalignment failure, i.e. in the system there exists an inner optimiser whose goals may be different to the base optimiser's goals. If this is the case, even if we select for models which appear to seek our base objective, the objective they actually end up pursuing may be different. These systems are deceptively aligned.</p>
<p>In this framing, the main way we might get deceptive alignment is through a system that performs internal search. While search is a central example, I'm curious to understand more generally what the prerequisites are for deceptive alignment. I'm particularly interested in the connection with consequentialism. In a sense, deception is just one of the many strategies a consequentialist agent might use to achieve its goals.</p>
<h2 id="how-does-the-idea-of-optimisation-matter-here">How does the idea of optimisation matter here?<a class="zola-anchor" href="#how-does-the-idea-of-optimisation-matter-here" aria-label="Anchor link for: how-does-the-idea-of-optimisation-matter-here" style="visibility: hidden;"></a>
</h2>
<p>In this article, I use "optimisation" in the context of an <a rel="nofollow noreferrer" href="https://www.lesswrong.com/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1">optimising system</a> – a system which tends to reconfigure a space toward a small set of target states, while being robust to perturbation. This does not require the existence of a part of the system which is doing the optimisation, i.e. a separate optimiser. In this view, I think it's possible to have a continuum of optimising systems, ranging from search over possible actions on one extreme to a set of heuristics on the other, such that all these systems outwardly behave as consequentialists, regardless of how they're implemented.</p>
<p>This is related to the view that some folks at MIRI have that consequentialism is about the work that's being done, rather than the type of system that is carrying it out. If that is the case, then in the limit of generalisation capability (i.e. systems which exhibit robust generalisation), it doesn't matter how the system is implemented – specifically, whether it's doing search or not – for how likely it is to exhibit deceptive behaviour. Note that my impression of Eliezer's view on this is that he does think that you can build a weaker system which doesn't exhibit some undesirable property such as deception, but that this weaker system is just not enough to carry out a pivotal act. (I base this mostly on <a rel="nofollow noreferrer" href="https://www.lesswrong.com/s/n945eovrA3oDueqtq/p/7im8at9PmhbT4JHsW">Eliezer's and Richard's conversation</a>.)</p>
<h1 id="what-properties-are-upstream-of-deception">What properties are upstream of deception?<a class="zola-anchor" href="#what-properties-are-upstream-of-deception" aria-label="Anchor link for: what-properties-are-upstream-of-deception" style="visibility: hidden;"></a>
</h1>
<p>Let's assume that some system has a consequentialist objective. This system lies somewhere on an axis of generalisation capability. On one extreme of this axis are <strong>controllers</strong>, systems which can be arbitrarily capable at carrying out their intended purpose, but which cannot extrapolate to other tasks. By "arbitrarily capable" I mean to hint that controllers can be very complex, i.e. they may have to carry out quite difficult tasks, by our standards, while still not being able to generalise to other tasks. One exception might be if those tasks are subtasks of their initial task, in the same way "boil water" is a subtask of a robot that makes coffee.</p>
<p>In my model, controllers don't "want" anything, not even with respect to accomplishing the task they have. Thermostats don't want to regulate temperature; they just have a set of rules and heuristics that apply in particular scenarios. If the thermostat has a rule: "if the temperature is under 20C, turn on the heating", there's no sense in which it "wants" to turn on the heating when the room is cold; it just does so. If this rule were removed and there were no other rules looking at the same scenario, the thermostat wouldn't try to find a way around the removal and still turn the heating on; it would not even know it needed to. It seems unlikely that controllers would be deceptively aligned.</p>
<p>On the other extreme of the axis are optimisers. These are systems which can generalise to states that were not encountered during the training process. In particular, they can generalise an objective of the type "do whatever it takes to achieve X" from states seen during training to states encountered during deployment. These systems can be deceptively aligned.</p>
<p>We kind of understand what systems look like in these extremes, but it's unclear to me what characteristics intermediate systems – those which are some mix of searching and heuristic behaviour – have. It seems important to find this out, given that neural networks seem to be somewhere on this continuum. If scaling deep learning leads to systems which generalise better – and thus are more like optimisers – then we'll hit a point where deceptive alignment is "natural".</p>
<h1 id="disentangling-generalisation-and-consequentialism">Disentangling generalisation and consequentialism<a class="zola-anchor" href="#disentangling-generalisation-and-consequentialism" aria-label="Anchor link for: disentangling-generalisation-and-consequentialism" style="visibility: hidden;"></a>
</h1>
<p>Here's an idea: to have systems which are deceptively aligned, you need them to both have a consequentialist objective and have high generalisation capability.</p>
<p>I'll continue with the same axis I described in the previous section. On one end are low-generalisation systems like look-up tables, expert systems and bags of heuristics. On the other, are searchy, optimiser-like systems, which generalise well to unseen states. Another way to think about this is: on one extreme, the outer training loop does the heavy lifting. All strategies are learned during the training process, and there is no mechanism by which a system can learn after deployment. On the other, the inner loop does most of the work; it finds new strategies at runtime, probably using search. Things in the middle use a combination of memorisation of data, rules, heuristics learned during training, as well as some degree of in-context/online learning. GPT seems to be somewhere between lookup tables and AIXI, probably closer to the former.</p>
<p>To see how deception arises in my model, I want to introduce a new axis. On this axis, systems range from "has a non-consequentialist objective" to "has a consequentialist objective". Here, the objective is the objective of the system. In the case of a controller, it's a behavioural objective, in the sense that we could accurately model the controller's behaviour as "trying to achieve X". For optimisers, the objective refers to their mesa-objective. This coordinate system also implies a "base objective" axis – this is not the same as the y-axis in the figure.</p>
<p><img src="/images/deceptive_systems_chart.png" alt="2D coordinate system showing generalisation capability vs consequentialism" /></p>
<p>A 2D system of coordinates: generalisation capability against consequentialism in the behavioural objective.</p>
<p>I want to examine four cases, corresponding to four imaginary quadrants (I don't think there will be a meaningful boundary, nor do I think that the quadrants are drawn realistically, i.e. that GPT is LGNC. I'm using this for the sake of analysis):</p>
<ol>
<li><strong>Low-generalisation, non-consequentialist objective</strong> <strong>(LGNC)</strong> systems: designed to achieve a task that is relatively simple, and which cannot extrapolate past that task. For example, a trivia system that memorises and retrieves facts about the world, without modelling agents.</li>
<li><strong>Low-generalisation, consequentialist objective</strong> <strong>(LGC)</strong> systems: designed to achieve a complex task that requires completing lots of smaller steps, and potentially figuring out what those steps are. An example here is a tabular policy learned using RL.</li>
<li><strong>High-generalisation, non-consequentialist objective (HGNC)</strong> systems: searchy things which accomplish some relatively simple task, like retrieving a fact about the world without modelling agents.</li>
<li><strong>High-generalisation, consequentialist objective</strong> <strong>(HGC)</strong> systems: searchy things with complex goals which require planning and achieving other subtasks, not being turned off or modified, the usual.</li>
</ol>
<p>I'm going to make a bunch of claims regarding these four quadrants/regimes, all of which are intuitions rather than hypotheses which I currently have lots of evidence for. Here goes:</p>
<ul>
<li>LGNC systems cannot be deceptive except through Goodharting on the task, as discussed <a rel="nofollow noreferrer" href="https://www.lesswrong.com/posts/8whGos5JCdBzDbZhH/framings-of-deceptive-alignment">here</a>. This is because these systems can't generalise beyond what they were trained on, and cannot discover deceptive strategies at runtime, through search. For these systems, their capability comes largely from the training process. LGNC systems are safe but useless.</li>
<li>Similarly, LGC can only carry out deception if they observed it in the training data. Because they have low generalisation capability, they can't extrapolate or recompose it from examples which did not show deception.</li>
<li>HGNC systems can be deceptive, but they are somewhat less dangerous than their consequentialist counterparts. These approximately map onto tool AIs. It seems like these aren't a central problem, as there are <a rel="nofollow noreferrer" href="https://www.gwern.net/Tool-AI">economic incentives</a> for HGC systems to develop rather than HGNC systems.</li>
<li>Deception <em>does arise</em> when moving from HGNC to HGC systems (acquiring consequentialism – "motive") or from LGC to HGC systems (acquiring generalisation power/search capabilities – "means"). These systems are very likely to be deceptive, if deception turns out to be a useful strategy for achieving their goals. These systems behave coherently and can be described as maximising some utility function.</li>
</ul>
<p>Insofar as these two axes are separate, I imagine that there's something like a Pareto front in the space of possible models, such that we can get most of our capability for the least amount of risk of deception possible. This is the way in which I see further work in this area to be impactful.</p>
<h1 id="where-this-falls-short">Where this falls short<a class="zola-anchor" href="#where-this-falls-short" aria-label="Anchor link for: where-this-falls-short" style="visibility: hidden;"></a>
</h1>
<p>There are various things here that confuse me, the main being what the x-axis represents. There is something unsatisfying about saying that deception happens because of generalisation capability and calling it a day. I'm interested if we can pinpoint deception even more precisely, in terms of properties like Ajeya's <a rel="nofollow noreferrer" href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">situational awareness</a> ("playing the training game"), or goal-directedness. In other words, what mechanisms enable deceptive alignment, and where do they emerge as a system becomes more capable?</p>
<p>Another confusion regarding this 2D system idea is that the two axes are not orthogonal. In my view, it's not possible to become more capable and not become consequentialist at all. This is because systems become more capable as they are trained on scenarios that are more varied, more broad, and more complex. That gets you consequentialism. Still, even if the axes are correlated, it doesn't mean that all systems lie on the first diagonal, and for every increase in capability you get a proportional increase in consequentialism.</p>
<h1 id="appendix-degree-of-coupling">Appendix: Degree of coupling?<a class="zola-anchor" href="#appendix-degree-of-coupling" aria-label="Anchor link for: appendix-degree-of-coupling" style="visibility: hidden;"></a>
</h1>
<p>It seems like an axis which overlaps to a significant degree with generalisation ability is the degree of internal coupling of the system. On the low-coupling extreme, each of a system's components is practically independent from the others. Their effects can chain together, such that after component A fulfils its purpose, component B can activate if its conditions are met. But there are no situations where in order for the system to achieve some behaviour two components need to fire at the same time, or be in two particular states, or interact in a specific way.</p>
<p>Moving toward the right on this axis, we get systems which are more coupled. There are behaviours which require more complex interactions between components, and these incentivise a sort of internal restructuring. For example, it's likely that components are reused to produce distinct behaviours: A + X = B1, A + Y = B2. I think it's the case that some components become very general, and so are used more often. Others remain specialised, and are only seldom used.</p>
<p>In my model, the further along this axis a system is, the more it relies on general components rather than case-specific heuristics and rules. A fully coupled system is one which uses only a handful of components which always interact in complex ways. In an extreme case, a system may use just one component – for example an exhaustive search over world states which maximise a utility function.</p>
<p>If coupling is something we observe in more capable systems, it might have implications for e.g. how useful our interpretability techniques are for detecting particular types of cognition.</p>

      </article>

      
      

      
      
      <div class="giscus"></div>
      
      
    </div>

    


<footer>
  <div class="left">
    <div class="copyright">
      © 2025 Andrei Alexandru
      
      <span>|</span>
      Built with <a href="https://www.getzola.org" rel="noreferrer" target="_blank">zola</a> and <a href="https://github.com/isunjn/serene" rel="noreferrer" target="_blank">serene</a>
      
    </div>
  </div>

  <div class="right">
    
    
      
    
    
    <a id="rss-btn" href="https://hd54s5ck6r3jexi5.preview.dev.igent.ai/posts/feed.xml">RSS</a>
    
    

    
    
    
    <button id="theme-toggle" aria-label="theme switch">
      <span class="moon-icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18" height="18"><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill="currentColor"></path></svg>
</span>
      <span class="sun-icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18" height="18"><path d="M12 18C8.68629 18 6 15.3137 6 12C6 8.68629 8.68629 6 12 6C15.3137 6 18 8.68629 18 12C18 15.3137 15.3137 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16ZM11 1H13V4H11V1ZM11 20H13V23H11V20ZM3.51472 4.92893L4.92893 3.51472L7.05025 5.63604L5.63604 7.05025L3.51472 4.92893ZM16.9497 18.364L18.364 16.9497L20.4853 19.0711L19.0711 20.4853L16.9497 18.364ZM19.0711 3.51472L20.4853 4.92893L18.364 7.05025L16.9497 5.63604L19.0711 3.51472ZM5.63604 16.9497L7.05025 18.364L4.92893 20.4853L3.51472 19.0711L5.63604 16.9497ZM23 11V13H20V11H23ZM4 11V13H1V11H4Z" fill="currentColor"></path></svg>
</span>
    </button>
    
  </div>
</footer>




<dialog id="rss-mask">
  <div>
    <a href="https:&#x2F;&#x2F;hd54s5ck6r3jexi5.preview.dev.igent.ai&#x2F;posts&#x2F;feed.xml">https:&#x2F;&#x2F;hd54s5ck6r3jexi5.preview.dev.igent.ai&#x2F;posts&#x2F;feed.xml</a>
    
    
    <button autofocus aria-label="copy" data-link="https:&#x2F;&#x2F;hd54s5ck6r3jexi5.preview.dev.igent.ai&#x2F;posts&#x2F;feed.xml" data-copy-icon="&lt;svg xmlns=&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&quot; viewBox=&quot;0 0 24 24&quot; width=&quot;18&quot; height=&quot;18&quot;&gt;&lt;path d=&quot;M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z&quot; fill=&quot;currentColor&quot;&gt;&lt;&#x2F;path&gt;&lt;&#x2F;svg&gt;
" data-check-icon="&lt;svg xmlns=&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&quot; viewBox=&quot;0 0 24 24&quot; width=&quot;18&quot; height=&quot;18&quot;&gt;&lt;path d=&quot;M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z&quot; fill=&quot;currentColor&quot;&gt;&lt;&#x2F;path&gt;&lt;&#x2F;svg&gt;
" >
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18" height="18"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>

    </button>
  </div>
</dialog>



  </main>
</div>

  
<script src="/js/lightense.min.js"></script>


  <script src="https://hd54s5ck6r3jexi5.preview.dev.igent.ai/js/main.js"></script>
</body>

</html>
