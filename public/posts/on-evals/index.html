<!DOCTYPE html>

<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="color-scheme" content="light dark">
  
  
    
  
  <meta name="description" content="Discussing recent work on evaluating large language models.">

  <title>On evals</title>
  <link rel="icon" type="image/png" sizes="32x32" href="https://jhpl6vjuqdkh13kz.preview.dev.igent.ai/img/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="https://jhpl6vjuqdkh13kz.preview.dev.igent.ai/img/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="https://jhpl6vjuqdkh13kz.preview.dev.igent.ai/img/apple-touch-icon.png">
  
  <style>

  /* light mode colors */
  body {
    --primary-color: #5871a2;
    --primary-pale-color: #5871a233;
    --primary-decoration-color: #5871a210;
    --bg-color: #ffffff;
    --text-color: #2f3030;
    --text-pale-color: #767676;
    --text-decoration-color: #a9a9a9;
    --highlight-mark-color: #5f75b020;

    --callout-note-color: #5871a2;
    --callout-tip-color: #268556;
    --callout-important-color: #885fc9;
    --callout-warning-color: #ab6632;
    --callout-caution-color: #c64e4e;
  }

  /* dark mode colors */
  body.dark {
    --primary-color: #6f8fd1;
    --primary-pale-color: #6f8fd166;
    --primary-decoration-color: #6f8fd112;
    --bg-color: #1c1c1c;
    --text-color: #c1c1c1;
    --text-pale-color: #848484;
    --text-decoration-color: #5f5f5f;
    --highlight-mark-color: #8296cb3b;

    --callout-note-color: #6f8fd1;
    --callout-tip-color: #47976f;
    --callout-important-color: #9776cd;
    --callout-warning-color: #ad7a52;
    --callout-caution-color: #d06161;
  }

  /* typography */
  body {
    --main-font: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
    --code-font: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace;
    --homepage-max-width: 768px;
    --main-max-width: 768px;
    --avatar-size: 60px;
    --font-size: 16px;
    --line-height: 1.75;
    --img-border-radius: 0px;
    --detail-border-radius: 0px;
    --dark-mode-img-brightness: 0.75;
    --dark-mode-chart-brightness: 0.75;
    --inline-code-border-radius: 2px;
    --inline-code-bg-color: var(--primary-decoration-color);
    --block-code-border-radius: 0px;
    --block-code-border-color: var(--primary-color);
    --detail-border-color: var(--primary-color);
  }

</style>

  <link rel="stylesheet" href="https://jhpl6vjuqdkh13kz.preview.dev.igent.ai/main.css">
  

<link id="hl" rel="stylesheet" type="text/css" href="/hl-light.css" />


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" integrity="sha384-HORx6nWi8j5/mYA+y57/9/CZc5z8HnEw4WUZWy5yOn9ToKBv1l58vJaufFAn9Zzi" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                { left: '$$', right: '$$', display: true },
                { left: '$', right: '$', display: false },
                { left: '\\(', right: '\\)', display: false },
                { left: '\\[', right: '\\]', display: true }
            ],
            throwOnError: false
        });
    });
</script>


  
</head>

<body class="post">
  
  <script>
    const theme = sessionStorage.getItem('theme');
    const match = window.matchMedia("(prefers-color-scheme: dark)").matches
    if ((theme && theme == 'dark') || (!theme && match)) {
      document.body.classList.add('dark');
      const hl = document.querySelector('link#hl');
      if (hl) hl.href = 'https://jhpl6vjuqdkh13kz.preview.dev.igent.ai/hl-dark.css';
    }
  </script>
  
  
<div id="wrapper">
  <div id="blank"></div>
  <aside>
    
    
    <nav>
      <ul>
        
        <li>
          <a class="h2" href="#persuasion-makemepay-theory-of-mind">Persuasion: MakeMePay &amp; Theory of Mind</a>
          
        </li>
        
        <li>
          <a class="h2" href="#agency-skill-acquisition">Agency: Skill Acquisition</a>
          
        </li>
        
        <li>
          <a class="h2" href="#hypothesis-generation-testing-20-questions">Hypothesis generation &amp; testing: 20 Questions</a>
          
        </li>
        
        <li>
          <a class="h2" href="#wrapping-up">Wrapping up</a>
          
        </li>
        
      </ul>
    </nav>
    
    
    <button id="back-to-top" aria-label="back to top">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-up"><line x1="12" y1="19" x2="12" y2="5"></line><polyline points="5 12 12 5 19 12"></polyline></svg>

    </button>
    
  </aside>
  <main>
    
<header>
  <nav>
    <a id="back-link" href="https:&#x2F;&#x2F;jhpl6vjuqdkh13kz.preview.dev.igent.ai&#x2F;posts">← Back</a>
  </nav>
</header>


    <div>
      
      
      
      
      <div id="copy-cfg" style="display: none;" data-copy-icon="&lt;svg xmlns=&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&quot; viewBox=&quot;0 0 24 24&quot; width=&quot;18&quot; height=&quot;18&quot;&gt;&lt;path d=&quot;M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z&quot; fill=&quot;currentColor&quot;&gt;&lt;&#x2F;path&gt;&lt;&#x2F;svg&gt;
" data-check-icon="&lt;svg xmlns=&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&quot; viewBox=&quot;0 0 24 24&quot; width=&quot;18&quot; height=&quot;18&quot;&gt;&lt;path d=&quot;M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z&quot; fill=&quot;currentColor&quot;&gt;&lt;&#x2F;path&gt;&lt;&#x2F;svg&gt;
"></div>
      
      <article class="prose">
        <h1>On evals</h1>
        <div id="post-info">
          <div id="date">
            <span id="publish">May 1, 2024</span>
            </div>

          
        </div>

        
        

        

        <p>Over the last year or so, I’ve been working on evaluating large language models’ capabilities, especially from a safety perspective. I think evals are particularly impactful at the moment, because there is increasing scrutiny of LMs from stakeholders outside the main labs – such as the government. Evals help shed some light on how models behave, especially as capabilities increase. They can also help make the case for slowing down or pausing development, for example by outlining criteria that trigger these (costly) actions.</p>
<p>I've open-sourced some of the evals I’ve worked on, as part of the <a rel="nofollow noreferrer" href="https://github.com/openai/evals">openai/evals</a> library. In this article I’ll walk you through the setups of these evals, and briefly discuss what they are intended to measure. The article covers two evals that focus on persuasion, and another two that broadly target research skills.</p>
<h2 id="persuasion-makemepay-theory-of-mind">Persuasion: MakeMePay &amp; Theory of Mind<a class="zola-anchor" href="#persuasion-makemepay-theory-of-mind" aria-label="Anchor link for: persuasion-makemepay-theory-of-mind" style="visibility: hidden;"></a>
</h2>
<p>The term persuasion has several meanings; I operationalise it as the ability to convince people of a particular statement, or to get them to take an action they might not otherwise have taken. I see persuasion as distinctly dual-use: you could use it to get people to revisit their misconceptions and biases, or you could convince them that <code>$OUTGROUP</code> is the source of all their problems. Ideally language models could do the former but not the latter, even when an LM is deployed by a motivated actor for that exact purpose. To help put safeguards in place, it’s helpful to be able to measure the ability of foundation models to persuade. It’s difficult to fully disentangle “ability to persuade” from more benign adjacent concepts, like “ability to be eloquent and well-reasoned”, but there are environments where this class of capabilities can be tested directly.</p>
<p><img src="/images/evals/makemepay.png" alt="MakeMePay" /></p>
<p><em>An example interaction from the MakeMePay eval. Two models, a con-artist and a mark, interact through dialogue. The con-artist tries to get the mark to give up money.</em></p>
<p>One of the public evals I want to cover is <a rel="nofollow noreferrer" href="https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay">MakeMePay</a>. It’s inspired by work from Google Deepmind on MakeMeSay, which is also <a rel="nofollow noreferrer" href="https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say">implemented in the evals repo</a>. MMS is a conversation between a language model and a user, where the model attempts to get the user to say a secret word. The user does not know that this is why the interaction is taking place, nor do they know the word. The model “wins” if the user says the word within a certain amount of replies; otherwise they fail.</p>
<p>MMP is similar, except it’s styled as an interaction between a “con-artist” and a “mark”. The con-artist – a language model – must convince the user to give up some amount of money. The idea is that more persuasive models will extract more money, faster. There are of course confounders here, such as if the mark is particularly open to giving up part of their money, total amount available, how the interaction is framed, and so on. But all else equal, you should expect to see a strong correlation between metrics like number of donations, amount extracted, conversation length, and overall model persuasion capability.</p>
<p>The eval tracks occasions where the mark gives up money by watching for a simple flag occurring in the conversation ([GIVE X]). Then, data is gathered on the amount donated and number of replies until a donation occurs, as well as tracking other metrics on e.g. whether the mark chooses to withdraw from the conversation (which signals that they might be onto the con-artist’s goal).</p>
<p>Gathering data for this is fairly expensive; one way to automate this eval and make it cheap to run is by replacing the human user with another language model. That is, instead of a human having a dialogue with a language model, you have a model-to-model interaction, with LMs playing both roles. This also allows you to control for characteristics of the mark through prompting – for example by making it more guarded and apprehensive of efforts to extract their money.</p>
<p>Many classes of evals, like MMS and MMP, can be implemented as dialogues between two or more participants. There are several features in the evals library that make it easy to create an eval based on this dialogue template. For example, the Solvers abstraction is used to decouple the requirements of an evaluation from the behaviour of the language model being evaluated. Solvers allow you to easily implement behaviour that isn’t just an inference call to an API – but which can have more complex logic like chain-of-thought and other prompting techniques. The evals library currently supports running evaluations on Gemini, Claude and open-source models through the <a rel="nofollow noreferrer" href="https://www.together.ai/">Together API</a>, alongside OpenAI models.</p>
<p>Stepping back, one question you could ask is whether it’s possible to measure other traits which feed into, or correlate well with e.g. high persuasiveness. These precursors can themselves be considered dual-use, but more often than not they look like useful, general purpose capabilities. For example, you wouldn’t consider “doing maths” as dual-use because it could be used in breaking cryptographic protocols.</p>
<p><a rel="nofollow noreferrer" href="https://wikipedia.org/wiki/Special:Search/theory%20of%20mind">Theory of mind</a> might be one such precursor. ToM is the ability to infer other people’s mental states, for example by considering what they know about their environment, how they’re feeling physically, what they might be thinking about given recent events, etc. Understanding others’ mental states seems instrumental to being more persuasive. It’s a form of information asymmetry: the persuader knows something about their interlocutor that helps them generate a more convincing, targeted argument. More speculatively, I would say that superhuman theory of mind probably looks like mind-reading.</p>
<p><img src="/images/evals/tom.png" alt="Theory of Mind" />
<em>Example data points from the three datasets used in the Theory of Mind eval. Hi-ToM extends the classic Sally-Anne test to evaluate higher-order beliefs (where does X think Y think…), while ToM adds distractor statements. SocialIQA focusses on common-sense reasoning in social interactions, which has a lot of overlap with the idea of theory of mind.</em></p>
<p>The <a rel="nofollow noreferrer" href="https://github.com/openai/evals/tree/main/evals/elsuite/theory_of_mind">Theory of Mind</a> eval is based on pre-existing literature on theory of mind in language models, using three publicly available datasets: <a rel="nofollow noreferrer" href="https://github.com/facebookresearch/ToMi">ToMi</a>, <a rel="nofollow noreferrer" href="https://lit.eecs.umich.edu/Hi-ToM/">Hi-ToM</a> (higher-order theory of mind), and <a rel="nofollow noreferrer" href="https://maartensap.com/social-iqa/">SocialIQA</a>. The first two are part of a common approach for assessing theory of mind in children using the <a rel="nofollow noreferrer" href="https://en.wikipedia.org/wiki/Sally%E2%80%93Anne_test">Sally-Anne test</a>. The test is structured as a set of observations regarding two people (Sally and Anne) followed by questions which assess the child’s memory and their ability to form first-order beliefs: beliefs about other agents’ beliefs. Both datasets introduce several improvements over the original case, for example by adding distractor observations, and by assessing higher-order beliefs (”What do you think that Sally thinks that Anne thinks about…”).
SocialIQA is focussed on “common-sense reasoning about social situations,” which have strong overlap with the above operationalisation of theory of mind. It is the largest of the three datasets, with 38,000 multiple-choice questions.</p>
<p>The evals library makes it straightforward to add dataset-only evals like ToM: you contribute the dataset as a JSONL file, and register your eval in a simple YAML config. There are several eval types ready to use out of the box, like exact- or fuzzy-matching (i.e. on the answers you provide in the dataset). For a quick walkthrough, see <a rel="nofollow noreferrer" href="https://github.com/openai/evals/blob/main/docs/build-eval.md">this readme</a>.</p>
<h2 id="agency-skill-acquisition">Agency: Skill Acquisition<a class="zola-anchor" href="#agency-skill-acquisition" aria-label="Anchor link for: agency-skill-acquisition" style="visibility: hidden;"></a>
</h2>
<p>Another skill that seems worth tracking is somewhere between autonomy and agency: the ability of a model to take charge of a particular task and execute it, including doing any intermediate steps. This is a fairly general concept; in the <a rel="nofollow noreferrer" href="https://github.com/openai/evals/tree/main/evals/elsuite/skill_acquisition">Skill Acquisition</a> eval, it’s operationalised as the ability to learn a new skill with minimal human involvement.</p>
<p><img src="/images/evals/skill_acquisition.png" alt="SkillAcquisition" />
<em>The Skill Acquisition eval tests a model’s ability to learn a new skill with minimal human involvement. The model answers questions about Miskito, an indigenous language, while having access to both relevant and distracting articles in a knowledge base.</em></p>
<p>The setup for SkillAcq is reminiscent of retrieval-augmented generation (RAG) which is becoming more and more common, e.g. through the Assistants API or via other application logic built on top of foundation models. From the perspective of the user it matters less whether the foundation model “learns” their documentation or just uses some scaffolding to access it at inference time. But here, it’s interesting to evaluate if the language model is able to drive its own interactions with a knowledge base. Specifically:</p>
<ul>
<li>the model prompt contains a list of files available to it, some of which are related to the task, while others are convincing-but-unhelpful distractor files;</li>
<li>the model interacts with the environment via dialogue, which enables it to select a file to browse or go to a specific section in a file;</li>
<li>the model can generate an answer at any point, even without looking at any of the files, by outputting the [ANSWER X] flag;</li>
</ul>
<p>The task is simple: the model answers questions about an indigenous language called <a rel="nofollow noreferrer" href="https://en.wikipedia.org/wiki/Miskito_language">Miskito</a>. I chose Miskito because it is likely poorly represented in language models’ training data, since online information about Miskito is sparse. The questions come from a publicly available, community-led Miskito language course, which across 10 units builds up a basic understanding of the language itself. Some of the languages involve translating from Miskito to English (and back), others are manipulations of sentences in Miskito, e.g. negations, or changing the object. As the model tries to answer these questions, it also has the study materials at hand, making the setup an open-book exam on Miskito.</p>
<p>A specific component of agency I’m interested in measuring here is the ability to recognise the limitations of one’s knowledge. Models often hallucinate knowledge they don’t have, which is a drawback for doing research with them. RAG helps by grounding the answer in a particular set of sources, but it doesn’t make the model itself more aware of its limits – it doesn’t know when to search or ask for help. I expect that future LMs being deployed as researchers will need something like this capability to be able to run without humans in the loop.</p>
<h2 id="hypothesis-generation-testing-20-questions">Hypothesis generation &amp; testing: 20 Questions<a class="zola-anchor" href="#hypothesis-generation-testing-20-questions" aria-label="Anchor link for: hypothesis-generation-testing-20-questions" style="visibility: hidden;"></a>
</h2>
<p>The last eval that I want to talk about here is <a rel="nofollow noreferrer" href="https://github.com/openai/evals/tree/main/evals/elsuite/twenty_questions">20 Questions</a>. It is a dialogue-based eval that is modelled after the game 20 questions, where one person – I call them the “gamemaster” – thinks of a word, while the other player has to guess it. The guesser can ask up to 20 yes-or-no questions about the word before they make a guess. In the setup, the gamemaster can also decline to answer if the question is ill-posed or ambiguous. The gamemaster is a fixed model – usually GPT-4 –, whereas the model playing the role of the guesser is varied. The main metric is a score defined as <code>20 - num_questions_asked</code>; the eval also tracks average number of questions, of guesses, of gamemaster refusals etc.
Alongside the core eval you’ll also find:</p>
<ul>
<li>an easier variant of the setup, where the guesser is given a shortlist of words from which the secret word was selected. In this variant, the guesser asks questions to rule out other words from the list, until only one remains.</li>
<li>a dataset comprising 207 nouns and their respective difficulties as rated by a team of contributors to the eval. Difficulties range from 1 to 3, with e.g. “pear” – 1, “fate” – 2, “prototype” – 3. The dataset also contains several proper nouns like “Star Wars” or “Cinderella.”</li>
</ul>
<p><img src="/images/evals/twenty_questions.png" alt="TwentyQuestions" />
<em>An example interaction from the 20 Questions eval. The gamemaster thinks of a word, and the player must guess the word in 20 questions or less.</em></p>
<p>20 questions aims to evaluate an ability which is often needed in the course of scientific research: generating and evaluating competing hypotheses. Similarly to how a researcher might come up with alternative explanations for an empirical phenomenon, or with different formulations of a theoretical framework, models playing 20 questions have to generate guesses about the word, and ask the right questions to guide their search.</p>
<h2 id="wrapping-up">Wrapping up<a class="zola-anchor" href="#wrapping-up" aria-label="Anchor link for: wrapping-up" style="visibility: hidden;"></a>
</h2>
<p>I hope this was interesting! If you want to run or extend these evals, have a look at the <a rel="nofollow noreferrer" href="https://github.com/openai/evals/tree/main">evals repo</a> and follow the instructions. I would be curious to hear feedback on your experience. There are currently many other open-source evals covering a pretty diverse set of capabilities. A few examples include <a rel="nofollow noreferrer" href="https://github.com/openai/evals/tree/main/evals/elsuite/sandbagging">Sandbagging</a>, <a rel="nofollow noreferrer" href="https://github.com/openai/evals/tree/main/evals/elsuite/self_prompting">Self Prompting</a>, and <a rel="nofollow noreferrer" href="https://github.com/openai/evals/tree/main/evals/elsuite/steganography">Steganography</a>. You can have a look at their README files <a rel="nofollow noreferrer" href="https://github.com/openai/evals/tree/main/evals/elsuite">here</a>.</p>

      </article>

      
      

      
      
      <div class="giscus"></div>
      
      
    </div>

    


<footer>
  <div class="left">
    <div class="copyright">
      © 2026 Andrei Alexandru
      
      <span>|</span>
      Built with <a href="https://www.getzola.org" rel="noreferrer" target="_blank">zola</a> and <a href="https://github.com/isunjn/serene" rel="noreferrer" target="_blank">serene</a>
      
    </div>
  </div>

  <div class="right">
    
    
      
    
    
    <a id="rss-btn" href="https://jhpl6vjuqdkh13kz.preview.dev.igent.ai/posts/feed.xml">RSS</a>
    
    

    
    
    
    <button id="theme-toggle" aria-label="theme switch">
      <span class="moon-icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18" height="18"><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill="currentColor"></path></svg>
</span>
      <span class="sun-icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18" height="18"><path d="M12 18C8.68629 18 6 15.3137 6 12C6 8.68629 8.68629 6 12 6C15.3137 6 18 8.68629 18 12C18 15.3137 15.3137 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16ZM11 1H13V4H11V1ZM11 20H13V23H11V20ZM3.51472 4.92893L4.92893 3.51472L7.05025 5.63604L5.63604 7.05025L3.51472 4.92893ZM16.9497 18.364L18.364 16.9497L20.4853 19.0711L19.0711 20.4853L16.9497 18.364ZM19.0711 3.51472L20.4853 4.92893L18.364 7.05025L16.9497 5.63604L19.0711 3.51472ZM5.63604 16.9497L7.05025 18.364L4.92893 20.4853L3.51472 19.0711L5.63604 16.9497ZM23 11V13H20V11H23ZM4 11V13H1V11H4Z" fill="currentColor"></path></svg>
</span>
    </button>
    
  </div>
</footer>




<dialog id="rss-mask">
  <div>
    <a href="https:&#x2F;&#x2F;jhpl6vjuqdkh13kz.preview.dev.igent.ai&#x2F;posts&#x2F;feed.xml">https:&#x2F;&#x2F;jhpl6vjuqdkh13kz.preview.dev.igent.ai&#x2F;posts&#x2F;feed.xml</a>
    
    
    <button autofocus aria-label="copy" data-link="https:&#x2F;&#x2F;jhpl6vjuqdkh13kz.preview.dev.igent.ai&#x2F;posts&#x2F;feed.xml" data-copy-icon="&lt;svg xmlns=&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&quot; viewBox=&quot;0 0 24 24&quot; width=&quot;18&quot; height=&quot;18&quot;&gt;&lt;path d=&quot;M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z&quot; fill=&quot;currentColor&quot;&gt;&lt;&#x2F;path&gt;&lt;&#x2F;svg&gt;
" data-check-icon="&lt;svg xmlns=&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&quot; viewBox=&quot;0 0 24 24&quot; width=&quot;18&quot; height=&quot;18&quot;&gt;&lt;path d=&quot;M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z&quot; fill=&quot;currentColor&quot;&gt;&lt;&#x2F;path&gt;&lt;&#x2F;svg&gt;
" >
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18" height="18"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>

    </button>
  </div>
</dialog>



  </main>
</div>

  
<script src="/js/lightense.min.js"></script>


  <script src="https://jhpl6vjuqdkh13kz.preview.dev.igent.ai/js/main.js"></script>
</body>

</html>
